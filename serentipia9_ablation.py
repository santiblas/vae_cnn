#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
serentipia9_ablation.py
"""
from __future__ import annotations
import optuna                     # ‚¨ÖÔ∏è  NUEVO  (lo usa el sampler)
from optuna.integration import OptunaSearchCV
import warnings
warnings.filterwarnings("ignore")
import warnings, logging
warnings.filterwarnings("ignore")
logging.getLogger("lightgbm").setLevel(logging.ERROR)
logging.getLogger("catboost").setLevel(logging.ERROR)
from optuna.integration import OptunaSearchCV
import logging
logging.getLogger("sklearn").setLevel(logging.ERROR)
logging.getLogger("lightgbm").setLevel(logging.ERROR)
optuna.logging.set_verbosity(optuna.logging.WARNING)
from models.convolutional_vae2 import ConvolutionalVAE
import numpy as np
import pandas as pd

from torch.cuda.amp import autocast, GradScaler # ‚¨ÖÔ∏è NUEVO
from pathlib import Path
import logging
from optuna.pruners import MedianPruner
import time
import cupy as cp
import torch.backends.cudnn as cudnn




import argparse
import xgboost as xgb 
import gc
import copy
import warnings
warnings.filterwarnings(
    "ignore",
    message=".*X does not have valid feature names.*",
    category=UserWarning,
    module="sklearn.utils.validation"
)
import subprocess
from models.classifiers7 import get_classifier_and_grid, get_available_classifiers
import joblib
from sklearn.model_selection import StratifiedKFold, train_test_split as sk_train_test_split, RepeatedStratifiedKFold
from sklearn.metrics import roc_auc_score, accuracy_score, recall_score, precision_score, f1_score, average_precision_score, balanced_accuracy_score
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt
from typing import List, Dict, Tuple, Optional
print("torch.cuda.is_available ->", torch.cuda.is_available())
print("torch.version.cuda      ->", torch.version.cuda)
print("cupy GPU visible        ->", cp.is_available())

# ‚Äî‚Äî‚Äî Configuraci√≥n GPU y cuDNN ‚Äî‚Äî‚Äî
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
torch.backends.cudnn.benchmark = True
torch.backends.cudnn.deterministic = False


# Opci√≥n 1 ‚Äì API estable desde CuPy 12
has_gpu = cp.is_available()              # True / False

# Opci√≥n 2 ‚Äì preguntar cu√°ntas GPU ve el runtime
try:
    has_gpu = cp.cuda.runtime.getDeviceCount() > 0
except cp.cuda.runtime.CUDARuntimeError:
    has_gpu = False

print("¬øGPU visible?:", has_gpu)

# 1Ô∏è‚É£  Deja TU logger en INFO
logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')
logger = logging.getLogger(__name__)

# 2Ô∏è‚É£  Silencia solo librer√≠as ruidosas
for noisy in ["lightgbm", "optuna", "sklearn", "xgboost"]:
    logging.getLogger(noisy).setLevel(logging.ERROR)

logger.info(f"Usando dispositivo: {device}")

# 3Ô∏è‚É£  Mant√©n los warnings desactivados si quieres
import warnings
warnings.filterwarnings("ignore")

# --- Constantes y Configuraciones Globales ---
DEFAULT_CHANNEL_NAMES = [
    'Pearson_OMST_GCE_Signed_Weighted', 'Pearson_Full_FisherZ_Signed', 'MI_KNN_Symmetric',
    'dFC_AbsDiffMean', 'dFC_StdDev', 'DistanceCorr', 'Granger_F_lag1' # <<< Lista actualizada para ser completa
]

FIXED_MINMAX_PARAMS_PER_CHANNEL = {}

def vae_loss_function(recon_x: torch.Tensor, x: torch.Tensor, mu: torch.Tensor, logvar: torch.Tensor, beta: float = 1.0) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    # Calcula la p√©rdida de reconstrucci√≥n (MSE)
    recon_loss = nn.functional.mse_loss(recon_x, x, reduction='sum') / x.shape[0]
    
    # Calcula la divergencia KL
    kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1).mean()
    
    # Calcula la p√©rdida total ponderada por beta
    total_loss = recon_loss + beta * kld_loss
    
    # Devuelve la p√©rdida total para el backward pass, y los componentes para el logging.
    # .detach() asegura que no se usen para calcular gradientes, solo para registro.
    return total_loss, recon_loss.detach(), kld_loss.detach() # ‚¨ÖÔ∏è CAMBIADO

def get_cyclical_beta_schedule(current_epoch: int, total_epochs: int, beta_max: float, n_cycles: int, ratio_increase: float = 0.5) -> float:
    if n_cycles <= 0: return beta_max
    epoch_per_cycle = total_epochs / n_cycles
    epoch_in_current_cycle = current_epoch % epoch_per_cycle
    increase_phase_duration = epoch_per_cycle * ratio_increase
    return beta_max * (epoch_in_current_cycle / increase_phase_duration) if epoch_in_current_cycle < increase_phase_duration else beta_max

def load_data(tensor_path: Path, metadata_path: Path) -> Tuple[Optional[np.ndarray], Optional[pd.DataFrame]]:
    logger.info(f"Cargando tensor global desde: {tensor_path}")
    if not tensor_path.exists():
        logger.error(f"Archivo de tensor global NO encontrado: {tensor_path}")
        return None, None
    try:
        data_npz = np.load(tensor_path)
        global_tensor = data_npz['global_tensor_data']
        subject_ids_tensor = data_npz['subject_ids'].astype(str) 
        logger.info(f"Tensor global cargado. Forma: {global_tensor.shape}")
    except Exception as e:
        logger.error(f"Error cargando tensor global: {e}")
        return None, None

    logger.info(f"Cargando metadatos desde: {metadata_path}")
    if not metadata_path.exists():
        logger.error(f"Archivo de metadatos NO encontrado: {metadata_path}")
        return None, None
    try:
        metadata_df = pd.read_csv(metadata_path)
        metadata_df['SubjectID'] = metadata_df['SubjectID'].astype(str).str.strip()
        logger.info(f"Metadatos cargados. Forma: {metadata_df.shape}")
    except Exception as e:
        logger.error(f"Error cargando metadatos: {e}")
        return None, None

    tensor_df = pd.DataFrame({'SubjectID': subject_ids_tensor})
    tensor_df['tensor_idx'] = np.arange(len(subject_ids_tensor))
    merged_df = pd.merge(tensor_df, metadata_df, on='SubjectID', how='left')
    
    num_tensor_subjects = len(subject_ids_tensor)
    if len(merged_df) < num_tensor_subjects:
         logger.warning(f"Algunos SubjectIDs del tensor ({num_tensor_subjects}) no se encontraron en los metadatos. Merged: {len(merged_df)}.")
    return global_tensor, merged_df

def normalize_inter_channel_fold(
    data_tensor: np.ndarray, 
    train_indices_in_fold: np.ndarray, 
    mode: str = 'zscore_offdiag',
    selected_channel_original_names: Optional[List[str]] = None
) -> Tuple[np.ndarray, List[Dict[str, float]]]:
    # Esta funci√≥n no cambia, su l√≥gica para normalizar el tensor VAE es correcta
    # y sigue operando sobre los datos de entrenamiento del VAE de cada fold.
    num_subjects_total, num_selected_channels, num_rois, _ = data_tensor.shape
    logger.info(f"Aplicando normalizaci√≥n inter-canal (modo: {mode}) sobre {num_selected_channels} canales seleccionados.")
    logger.info(f"Par√°metros de normalizaci√≥n se calcular√°n usando {len(train_indices_in_fold)} sujetos de entrenamiento.")
    
    normalized_tensor_fold = data_tensor.copy()
    norm_params_per_channel_list = []
    off_diag_mask = ~np.eye(num_rois, dtype=bool)

    for c_idx_selected in range(num_selected_channels):
        current_channel_original_name = selected_channel_original_names[c_idx_selected] if selected_channel_original_names and c_idx_selected < len(selected_channel_original_names) else f"Channel_{c_idx_selected}"
        params = {'mode': mode, 'original_name': current_channel_original_name}
        use_fixed_params = False

        if mode == 'minmax_offdiag' and current_channel_original_name in FIXED_MINMAX_PARAMS_PER_CHANNEL:
            fixed_p = FIXED_MINMAX_PARAMS_PER_CHANNEL[current_channel_original_name]
            params.update({'min': fixed_p['min'], 'max': fixed_p['max']})
            use_fixed_params = True
            logger.info(f"Canal '{current_channel_original_name}': Usando MinMax fijo (min={params['min']:.4f}, max={params['max']:.4f}).")

        if not use_fixed_params:
            channel_data_train_for_norm_params = data_tensor[train_indices_in_fold, c_idx_selected, :, :]
            all_off_diag_train_values = channel_data_train_for_norm_params[:, off_diag_mask].flatten()

            if all_off_diag_train_values.size == 0:
                logger.warning(f"Canal '{current_channel_original_name}': No hay elementos fuera de la diagonal en el training set. No se escala.")
                params.update({'mean': 0.0, 'std': 1.0, 'min': 0.0, 'max': 1.0, 'no_scale': True})
            elif mode == 'zscore_offdiag':
                mean_val = np.mean(all_off_diag_train_values)
                std_val = np.std(all_off_diag_train_values)
                params.update({'mean': mean_val, 'std': std_val if std_val > 1e-9 else 1.0})
                if std_val <= 1e-9: logger.warning(f"Canal '{current_channel_original_name}': STD muy bajo ({std_val:.2e}). Usando STD=1.")
            elif mode == 'minmax_offdiag':
                min_val = np.min(all_off_diag_train_values)
                max_val = np.max(all_off_diag_train_values)
                params.update({'min': min_val, 'max': max_val})
                if (max_val - min_val) <= 1e-9: logger.warning(f"Canal '{current_channel_original_name}': Rango (max-min) muy bajo ({(max_val - min_val):.2e}).")
            else:
                raise ValueError(f"Modo de normalizaci√≥n desconocido: {mode}")
        
        norm_params_per_channel_list.append(params)

        if not params.get('no_scale', False):
            current_channel_data_all_subjects = data_tensor[:, c_idx_selected, :, :]
            scaled_channel_data = current_channel_data_all_subjects.copy()
            if off_diag_mask.any():
                if mode == 'zscore_offdiag':
                    if params['std'] > 1e-9:
                        scaled_channel_data[:, off_diag_mask] = (current_channel_data_all_subjects[:, off_diag_mask] - params['mean']) / params['std']
                elif mode == 'minmax_offdiag':
                    range_val = params.get('max', 1.0) - params.get('min', 0.0)
                    if range_val > 1e-9: 
                        scaled_channel_data[:, off_diag_mask] = (current_channel_data_all_subjects[:, off_diag_mask] - params['min']) / range_val
                    else: 
                        scaled_channel_data[:, off_diag_mask] = 0.0 
            normalized_tensor_fold[:, c_idx_selected, :, :] = scaled_channel_data
            if not use_fixed_params:
                log_msg_params = f"Canal '{current_channel_original_name}': Off-diag {mode} (train_params: "
                if mode == 'zscore_offdiag': log_msg_params += f"mean={params['mean']:.3f}, std={params['std']:.3f})"
                elif mode == 'minmax_offdiag': log_msg_params += f"min={params['min']:.3f}, max={params['max']:.3f})"
                logger.info(log_msg_params)
    return normalized_tensor_fold, norm_params_per_channel_list


def apply_normalization_params(data_tensor_subset: np.ndarray, 
                               norm_params_per_channel_list: List[Dict[str, float]]
                               ) -> np.ndarray:
    # Esta funci√≥n tampoco cambia
    num_subjects, num_selected_channels, num_rois, _ = data_tensor_subset.shape
    normalized_tensor_subset = data_tensor_subset.copy()
    off_diag_mask = ~np.eye(num_rois, dtype=bool)

    if len(norm_params_per_channel_list) != num_selected_channels:
        raise ValueError(f"Mismatch in number of channels for normalization: data has {num_selected_channels}, params provided for {len(norm_params_per_channel_list)}")

    for c_idx_selected in range(num_selected_channels):
        params = norm_params_per_channel_list[c_idx_selected]
        mode = params.get('mode', 'zscore_offdiag') 
        if params.get('no_scale', False):
            continue
        current_channel_data = data_tensor_subset[:, c_idx_selected, :, :]
        scaled_channel_data_subset = current_channel_data.copy()
        if off_diag_mask.any():
            if mode == 'zscore_offdiag':
                if params['std'] > 1e-9:
                    scaled_channel_data_subset[:, off_diag_mask] = (current_channel_data[:, off_diag_mask] - params['mean']) / params['std']
            elif mode == 'minmax_offdiag':
                range_val = params.get('max', 1.0) - params.get('min', 0.0)
                if range_val > 1e-9:
                    scaled_channel_data_subset[:, off_diag_mask] = (current_channel_data[:, off_diag_mask] - params['min']) / range_val
                else:
                    scaled_channel_data_subset[:, off_diag_mask] = 0.0 
        normalized_tensor_subset[:, c_idx_selected, :, :] = scaled_channel_data_subset
    return normalized_tensor_subset

def log_group_distributions(df: pd.DataFrame, group_cols: List[str], dataset_name: str, fold_idx_str: str):
    # Sin cambios en esta funci√≥n
    if df.empty:
        logger.info(f"  {fold_idx_str} {dataset_name}: DataFrame vac√≠o.")
        return
    log_msg = f"  {fold_idx_str} {dataset_name} (N={len(df)}):\n"
    for col in group_cols:
        if col in df.columns:
            counts = df[col].value_counts().sort_index()
            log_msg += f"    {col}:\n"
            for val, count in counts.items():
                log_msg += f"      {val}: {count} ({count/len(df)*100:.1f}%)\n"
        else:
            log_msg += f"    {col}: No encontrado en el DataFrame.\n"
    logger.info(log_msg.strip())


def train_and_evaluate_pipeline(global_tensor_all_channels: np.ndarray, 
                                metadata_df_full: pd.DataFrame,
                                args: argparse.Namespace):
    
    # ... (la configuraci√≥n inicial del pipeline y del VAE no cambia) ...
    output_base_dir = Path(args.output_dir)
    output_base_dir.mkdir(parents=True, exist_ok=True)

    # <<< INICIO DE LA SECCI√ìN MODIFICADA Y CORREGIDA >>>
    
    # Si se pasan los √≠ndices de los canales a usar, se usan esos.
    if hasattr(args, 'channels_to_use') and args.channels_to_use is not None:
        selected_channel_indices = args.channels_to_use
        
        # Usa la lista completa de nombres pasada desde el script principal (ej. `all_original_channel_names`).
        # Si no se pasa, usa la lista por defecto como fallback.
        master_channel_list = getattr(args, 'all_original_channel_names', DEFAULT_CHANNEL_NAMES)
        
        try:
            # Intenta mapear los √≠ndices a los nombres usando la lista maestra
            selected_channel_names_in_tensor = [master_channel_list[i] for i in selected_channel_indices]
        except IndexError:
            logger.error(f"Error de √≠ndice al mapear nombres de canal. Se esperaba una lista de nombres de longitud {global_tensor_all_channels.shape[1]} pero se recibi√≥ una de longitud {len(master_channel_list)}. Usando nombres gen√©ricos.")
            # L√≥gica de fallback por si acaso
            selected_channel_names_in_tensor = [ f"RawChan{i}" for i in selected_channel_indices ]

        current_global_tensor = global_tensor_all_channels[:, selected_channel_indices, :, :]
        logger.info(f"Usando canales seleccionados (√≠ndices): {selected_channel_indices}")
        logger.info(f"Nombres de canales seleccionados: {selected_channel_names_in_tensor}")

    else:
        # L√≥gica original si no se especifica `channels_to_use`
        current_global_tensor = global_tensor_all_channels
        master_channel_list = getattr(args, 'all_original_channel_names', DEFAULT_CHANNEL_NAMES)
        selected_channel_names_in_tensor = [master_channel_list[i] if i < len(master_channel_list) else f"RawChan{i}" for i in range(current_global_tensor.shape[1])]
        logger.info(f"Usando todos los {current_global_tensor.shape[1]} canales.")
    
    
    num_input_channels_for_vae = current_global_tensor.shape[1]

    if 'ResearchGroup_Mapped' not in metadata_df_full.columns:
        logger.error("'ResearchGroup_Mapped' no encontrado. Abortando.")
        return
    cn_ad_df = metadata_df_full[metadata_df_full['ResearchGroup_Mapped'].isin(['CN', 'AD'])].copy()
    if cn_ad_df.empty or 'tensor_idx' not in cn_ad_df.columns:
        logger.error("No hay sujetos CN/AD o falta 'tensor_idx' en el DataFrame mergeado. Abortando.")
        return
    
    max_valid_idx_for_cn_ad = current_global_tensor.shape[0] - 1
    original_cn_ad_count = len(cn_ad_df)
    cn_ad_df = cn_ad_df[cn_ad_df['tensor_idx'] <= max_valid_idx_for_cn_ad].copy()
    if len(cn_ad_df) < original_cn_ad_count:
        logger.warning(f"Algunos sujetos CN/AD filtrados porque 'tensor_idx' excede las dimensiones del tensor. "
                       f"Original: {original_cn_ad_count}, Post-filtro: {len(cn_ad_df)}")

    if cn_ad_df.empty:
        logger.error("No hay sujetos CN/AD v√°lidos despu√©s de filtrar por tensor_idx. Abortando.")
        return

    cn_ad_df['label'] = cn_ad_df['ResearchGroup_Mapped'].map({'CN': 0, 'AD': 1})
    
    strat_cols = ['ResearchGroup_Mapped']
    if args.classifier_stratify_cols:
        for col in args.classifier_stratify_cols:
            if col in cn_ad_df.columns:
                # Asegurarse de que las columnas de estratificaci√≥n no tengan NaNs
                cn_ad_df[col] = cn_ad_df[col].fillna(f"{col}_Unknown").astype(str)
                if col not in strat_cols:
                    strat_cols.append(col)
            else:
                logger.warning(f"Columna de estratificaci√≥n para el clasificador '{col}' no encontrada.")

    cn_ad_df['stratify_key_clf'] = cn_ad_df[strat_cols].apply(lambda x: '_'.join(x.astype(str)), axis=1)
    logger.info(f"Estratificando folds del CLASIFICADOR por: {strat_cols}")

    X_classifier_subject_indices_in_cn_ad_df = np.arange(len(cn_ad_df))
    y_classifier_labels_cn_ad = cn_ad_df['label'].values
    stratify_key_for_clf_cv = cn_ad_df['stratify_key_clf']
    
    logger.info(f"Sujetos CN/AD para clasificaci√≥n: {len(cn_ad_df)}. CN: {sum(y_classifier_labels_cn_ad == 0)}, AD: {sum(y_classifier_labels_cn_ad == 1)}")

    if args.repeated_outer_folds_n_repeats > 1:
        outer_cv_clf = RepeatedStratifiedKFold(n_splits=args.outer_folds, n_repeats=args.repeated_outer_folds_n_repeats, random_state=args.seed)
        total_outer_iterations = args.outer_folds * args.repeated_outer_folds_n_repeats
    else:
        outer_cv_clf = StratifiedKFold(n_splits=args.outer_folds, shuffle=True, random_state=args.seed)
        total_outer_iterations = args.outer_folds
    logger.info(f"Usando CV externa: {type(outer_cv_clf).__name__} con {total_outer_iterations} iteraciones totales.")

    all_folds_metrics = []
    all_folds_vae_history = []
    all_folds_clf_predictions = []

    for fold_idx, (train_dev_clf_idx_in_cn_ad_df, test_clf_idx_in_cn_ad_df) in enumerate(outer_cv_clf.split(X_classifier_subject_indices_in_cn_ad_df, stratify_key_for_clf_cv)):
        fold_start_time = time.time()
        fold_idx_str = f"Fold {fold_idx + 1}/{total_outer_iterations}"
        logger.info(f"--- Iniciando {fold_idx_str} ---")
        # Dentro del bucle de folds en serentipia6.py
        
        
        # ... (la l√≥gica de entrenamiento del VAE hasta el guardado del modelo no cambia) ...
        fold_output_dir = Path(args.output_dir) / f"fold_{fold_idx + 1}"
        fold_output_dir.mkdir(parents=True, exist_ok=True)
        np.save(fold_output_dir / "test_indices.npy", test_clf_idx_in_cn_ad_df)

        global_indices_clf_test_this_fold = cn_ad_df.iloc[test_clf_idx_in_cn_ad_df]['tensor_idx'].values
        log_group_distributions(cn_ad_df.iloc[test_clf_idx_in_cn_ad_df], strat_cols, "Test Set (Clasificador)", fold_idx_str)

        all_valid_subject_indices_from_metadata = metadata_df_full[metadata_df_full['tensor_idx'] <= max_valid_idx_for_cn_ad]['tensor_idx'].values
        global_indices_vae_training_pool = np.setdiff1d(all_valid_subject_indices_from_metadata, global_indices_clf_test_this_fold, assume_unique=True)
        
        if len(global_indices_vae_training_pool) < 10: 
            logger.error(f"{fold_idx_str}: Muy pocos sujetos ({len(global_indices_vae_training_pool)}) para entrenamiento VAE. Saltando fold.")
            continue

        vae_train_pool_df = metadata_df_full[metadata_df_full['tensor_idx'].isin(global_indices_vae_training_pool)]
        log_group_distributions(vae_train_pool_df, ['ResearchGroup_Mapped', 'Sex', 'Age_Group'], "Pool Entrenamiento VAE", fold_idx_str)
        
        vae_train_pool_tensor_original_scale = current_global_tensor[global_indices_vae_training_pool]
        
        vae_actual_train_indices_local_to_pool, vae_internal_val_indices_local_to_pool = [], []

        # ‚ñº‚ñº‚ñº CAMBIO 3: Estratificaci√≥n robusta para el split de validaci√≥n del VAE ‚ñº‚ñº‚ñº
        stratify_key_vae_split = None
        # Columnas para un split de validaci√≥n del VAE bien balanceado.
        vae_strat_cols = ['ResearchGroup_Mapped', 'Sex', 'Age_Group'] 
        temp_vae_strat_df = vae_train_pool_df[vae_strat_cols].copy()

        # Rellenar NaNs y crear una clave √∫nica para la estratificaci√≥n del VAE
        for col in vae_strat_cols:
            if col not in temp_vae_strat_df.columns:
                logger.warning(f"Columna '{col}' no encontrada para la estratificaci√≥n del VAE. Se omitir√°.")
                vae_strat_cols.remove(col)
            else:
                temp_vae_strat_df[col] = temp_vae_strat_df[col].fillna(f"{col}_Unknown").astype(str)

        try:
            stratify_key_vae_split = temp_vae_strat_df.apply(lambda x: '_'.join(x), axis=1)
            # Comprobar si hay suficientes muestras en cada estrato para hacer el split
            if all(stratify_key_vae_split.value_counts() >= 2):
                logger.info(f"  {fold_idx_str} VAE val split ser√° estratificado por {vae_strat_cols}.")
            else:
                logger.warning(f"  {fold_idx_str} No hay suficientes muestras en cada estrato para el split del VAE. Usando solo 'ResearchGroup_Mapped'.")
                stratify_key_vae_split = vae_train_pool_df['ResearchGroup_Mapped']

        except Exception as e:
            logger.error(f"  {fold_idx_str} Error creando la clave de estratificaci√≥n del VAE: {e}. Se usar√° solo 'ResearchGroup_Mapped'.")
            stratify_key_vae_split = vae_train_pool_df['ResearchGroup_Mapped']

        if args.vae_val_split_ratio > 0 and len(global_indices_vae_training_pool) > 10:
            try:
                vae_actual_train_indices_local_to_pool, vae_internal_val_indices_local_to_pool = sk_train_test_split(
                    np.arange(len(global_indices_vae_training_pool)),
                    test_size=args.vae_val_split_ratio,
                    stratify=stratify_key_vae_split, # Usamos la nueva clave de estratificaci√≥n
                    random_state=args.seed + fold_idx + 10, shuffle=True
                )
            except ValueError as e:
                logger.error(f"  {fold_idx_str} Error al hacer el split de validaci√≥n del VAE: {e}. Usando todo el pool como train.")
                vae_actual_train_indices_local_to_pool = np.arange(len(global_indices_vae_training_pool))
                vae_internal_val_indices_local_to_pool = []
        # Correcci√≥n
        log_group_distributions(vae_train_pool_df.iloc[vae_actual_train_indices_local_to_pool], ['ResearchGroup_Mapped', 'Sex', 'Age_Group'], "Actual Train Set (VAE)", fold_idx_str)
        if len(vae_internal_val_indices_local_to_pool) > 0:
            log_group_distributions(vae_train_pool_df.iloc[vae_internal_val_indices_local_to_pool], ['ResearchGroup_Mapped', 'Sex', 'Age_Group'], "Internal Val Set (VAE)", fold_idx_str)
        #log_group_distributions(vae_train_pool_df.iloc[vae_actual_train_indices_local_to_pool], ['ResearchGroup_Mapped', 'Sex'], "Actual Train Set (VAE)", fold_idx_str)
        #if len(vae_internal_val_indices_local_to_pool) > 0:
        #     log_group_distributions(vae_train_pool_df.iloc[vae_internal_val_indices_local_to_pool], ['ResearchGroup_Mapped', 'Sex'], "Internal Val Set (VAE)", fold_idx_str)
        logger.info(f"  {fold_idx_str} Sujetos VAE actual train: {len(vae_actual_train_indices_local_to_pool)}, VAE internal val: {len(vae_internal_val_indices_local_to_pool)}")

        vae_pool_tensor_norm, norm_params_fold_list = normalize_inter_channel_fold(
            vae_train_pool_tensor_original_scale, vae_actual_train_indices_local_to_pool, 
            mode=args.norm_mode, selected_channel_original_names=selected_channel_names_in_tensor
        )
        # Dentro del bucle de folds en serentipia6.py
        joblib.dump(norm_params_fold_list, fold_output_dir / "vae_norm_params.joblib")
        vae_train_dataset = TensorDataset(torch.from_numpy(vae_pool_tensor_norm[vae_actual_train_indices_local_to_pool]).float())
        #vae_train_loader = DataLoader(vae_train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=True)
        vae_train_loader = DataLoader(
            vae_train_dataset,
            batch_size=args.batch_size,
            shuffle=True,
            num_workers=args.num_workers,
            pin_memory=True
        )
        vae_internal_val_loader = None
        if len(vae_internal_val_indices_local_to_pool) > 0:
            vae_internal_val_dataset = TensorDataset(torch.from_numpy(vae_pool_tensor_norm[vae_internal_val_indices_local_to_pool]).float())
            #vae_internal_val_loader = DataLoader(vae_internal_val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers, pin_memory=True)
            vae_internal_val_loader = DataLoader(
                vae_internal_val_dataset,
                batch_size=args.batch_size,
                shuffle=False,
                num_workers=args.num_workers,
                pin_memory=True
            )
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"  {fold_idx_str} Usando dispositivo: {device}")
        
        vae_fold_k = ConvolutionalVAE(
            input_channels=num_input_channels_for_vae, latent_dim=args.latent_dim, image_size=current_global_tensor.shape[-1],
            final_activation=args.vae_final_activation, intermediate_fc_dim_config=args.intermediate_fc_dim_vae,
            dropout_rate=args.dropout_rate_vae, use_layernorm_fc=args.use_layernorm_vae_fc,
            num_conv_layers_encoder=args.num_conv_layers_encoder, decoder_type=args.decoder_type
        ).to(device)
        
        optimizer_vae = optim.AdamW(vae_fold_k.parameters(), lr=args.lr_vae, weight_decay=args.weight_decay_vae, amsgrad=True)
        scheduler_vae = None
        # ‚ñº‚ñº‚ñº L√ìGICA DE SCHEDULER ACTUALIZADA ‚ñº‚ñº‚ñº
        if vae_internal_val_loader:
            if args.lr_scheduler_type == 'plateau':
                if args.lr_scheduler_patience_vae > 0:
                    logger.info(f"¬† {fold_idx_str} Usando scheduler: ReduceLROnPlateau")
                    scheduler_vae = optim.lr_scheduler.ReduceLROnPlateau(
                        optimizer_vae, 'min', 
                        patience=args.lr_scheduler_patience_vae, 
                        factor=0.1
                    )
            elif args.lr_scheduler_type == 'cosine_warm':
                logger.info(f"¬† {fold_idx_str} Usando scheduler: CosineAnnealingWarmRestarts (T_0={args.lr_scheduler_T0})")
                scheduler_vae = optim.lr_scheduler.CosineAnnealingWarmRestarts(
                    optimizer_vae, 
                    T_0=args.lr_scheduler_T0, 
                    eta_min=args.lr_scheduler_eta_min
                )
        # ... (c√≥digo previo de inicializaci√≥n del VAE) ...
        logger.info(f"  {fold_idx_str} Entrenando VAE (Decoder: {args.decoder_type}, Encoder Layers: {args.num_conv_layers_encoder})...")
        best_val_loss = float('inf')
        best_epoch = 0
        epochs_no_improve = 0
        best_model_state_dict = None
        
        # Listas para guardar el historial completo
        history_data = {
            "train_loss": [], "train_recon": [], "train_kld": [],
            "val_loss": [], "val_recon": [], "val_kld": [],
            "beta": []
        } # ‚¨ÖÔ∏è NUEVO: Diccionario centralizado

        scaler = GradScaler(enabled=(device.type == 'cuda'))

        for epoch in range(args.epochs_vae):
            vae_fold_k.train()
            # Acumuladores para la √©poca de entrenamiento
            epoch_train_loss, epoch_train_recon, epoch_train_kld = 0.0, 0.0, 0.0 # ‚¨ÖÔ∏è NUEVO
            current_beta = get_cyclical_beta_schedule(epoch, args.epochs_vae, args.beta_vae, args.cyclical_beta_n_cycles, args.cyclical_beta_ratio_increase)
            
            # ‚ñº‚ñº‚ñº BUCLE DE ENTRENAMIENTO MODIFICADO ‚ñº‚ñº‚ñº
            for i, (data,) in enumerate(vae_train_loader):
                data = data.to(device)
                optimizer_vae.zero_grad(set_to_none=True)

                with autocast(enabled=(device.type == 'cuda')):
                    recon_batch, mu, logvar, _ = vae_fold_k(data)
                    loss, recon, kld = vae_loss_function(recon_batch, data, mu, logvar, beta=current_beta)
                
                scaler.scale(loss).backward()
                scaler.step(optimizer_vae)
                scaler.update()

                # Actualizar el scheduler de coseno en cada paso
                if scheduler_vae and args.lr_scheduler_type == 'cosine_warm':
                    scheduler_vae.step(epoch + i / len(vae_train_loader)) # Actualizaci√≥n por paso

                epoch_train_loss += loss.item() * data.size(0)
                epoch_train_recon += recon.item() * data.size(0)
                epoch_train_kld += kld.item() * data.size(0)
            # ‚ñ≤‚ñ≤‚ñ≤ FIN BUCLE DE ENTRENAMIENTO MODIFICADO ‚ñ≤‚ñ≤‚ñ≤
            
            # Calculamos las medias y las guardamos en el historial
            history_data["train_loss"].append(epoch_train_loss / len(vae_train_loader.dataset))
            history_data["train_recon"].append(epoch_train_recon / len(vae_train_loader.dataset))
            history_data["train_kld"].append(epoch_train_kld / len(vae_train_loader.dataset))
            history_data["beta"].append(current_beta)
            
            log_msg = (f"  {fold_idx_str} VAE E{epoch+1}/{args.epochs_vae}, "
                       f"TrL: {history_data['train_loss'][-1]:.2f} "
                       f"(R: {history_data['train_recon'][-1]:.2f}, "
                       f"KLD: {history_data['train_kld'][-1]:.2f}), " # ‚¨ÖÔ∏è Log mejorado
                       f"Beta: {current_beta:.3f}, LR: {optimizer_vae.param_groups[0]['lr']:.2e}")

            if vae_internal_val_loader:
                vae_fold_k.eval()
                # Acumuladores para la √©poca de validaci√≥n
                epoch_val_loss, epoch_val_recon, epoch_val_kld = 0.0, 0.0, 0.0 # ‚¨ÖÔ∏è NUEVO
                with torch.no_grad():
                    with autocast(enabled=(device.type == 'cuda')):
                        for (val_data,) in vae_internal_val_loader:
                            val_data = val_data.to(device)
                            recon_val, mu_val, logvar_val, _ = vae_fold_k(val_data)
                            # Obtenemos los 3 componentes tambi√©n en validaci√≥n
                            v_loss, v_recon, v_kld = vae_loss_function(recon_val, val_data, mu_val, logvar_val, beta=current_beta) # ‚¨ÖÔ∏è CAMBIADO
                            
                            epoch_val_loss += v_loss.item() * val_data.size(0)
                            epoch_val_recon += v_recon.item() * val_data.size(0) # ‚¨ÖÔ∏è NUEVO
                            epoch_val_kld += v_kld.item() * val_data.size(0)     # ‚¨ÖÔ∏è NUEVO

                # Calculamos medias y guardamos en historial
                avg_epoch_val_loss = epoch_val_loss / len(vae_internal_val_loader.dataset)
                history_data["val_loss"].append(avg_epoch_val_loss)
                history_data["val_recon"].append(epoch_val_recon / len(vae_internal_val_loader.dataset))
                history_data["val_kld"].append(epoch_val_kld / len(vae_internal_val_loader.dataset))
                
                log_msg += (f", ValL: {history_data['val_loss'][-1]:.2f} "
                            f"(R: {history_data['val_recon'][-1]:.2f}, "
                            f"KLD: {history_data['val_kld'][-1]:.2f})") # ‚¨ÖÔ∏è Log mejorado

                # L√≥gica de scheduler y early stopping (sin cambios)
                old_lr = optimizer_vae.param_groups[0]['lr']
                if scheduler_vae and args.lr_scheduler_type == 'plateau':
                    scheduler_vae.step(avg_epoch_val_loss)

                #if scheduler_vae: scheduler_vae.step(avg_epoch_val_loss)
                new_lr = optimizer_vae.param_groups[0]['lr']
                
                if new_lr < old_lr:
                    logger.info(f"  {fold_idx_str} Learning rate reducido a {new_lr:.2e}. Reiniciando contador de early stopping.")
                    epochs_no_improve = 0

                if avg_epoch_val_loss < best_val_loss:
                    if not np.isnan(avg_epoch_val_loss):
                        best_val_loss = avg_epoch_val_loss
                        best_epoch = epoch + 1
                        epochs_no_improve = 0
                        best_model_state_dict = copy.deepcopy(vae_fold_k.state_dict())
                else:
                    epochs_no_improve += 1

                if args.early_stopping_patience_vae > 0 and epochs_no_improve >= args.early_stopping_patience_vae:
                    val_str = f"{best_val_loss:.4f}" if not np.isnan(best_val_loss) else "N/A"
                    logger.info(f"  {fold_idx_str} Early stopping VAE en epoch {epoch+1}. "
                        f"Mejor val_loss: {best_val_loss:.4f} (√©poca {best_epoch})")
                    break
            else: 
                 # Si no hay validaci√≥n, rellenamos con NaN para mantener la estructura
                 for key in ["val_loss", "val_recon", "val_kld"]:
                     history_data[key].append(np.nan) # ‚¨ÖÔ∏è CAMBIADO
                 best_model_state_dict = copy.deepcopy(vae_fold_k.state_dict()) 

            if (epoch + 1) % args.log_interval_epochs_vae == 0 or epoch == args.epochs_vae - 1:
                logger.info(log_msg)
        
        # ... El resto del c√≥digo contin√∫a
        
        if best_model_state_dict:
            vae_fold_k.load_state_dict(best_model_state_dict)
            val_string = f"{best_val_loss:.4f}" if vae_internal_val_loader and not np.isnan(best_val_loss) else "N/A - Last Epoch"
            logger.info(f"  {fold_idx_str} VAE final model loaded (best val_loss: {val_string}).")

        vae_model_fname = f"vae_model_fold_{fold_idx+1}.pt"
        torch.save(vae_fold_k.state_dict(), fold_output_dir / vae_model_fname)
        logger.info(f"  {fold_idx_str} Modelo VAE guardado en: {fold_output_dir / vae_model_fname}")
        
        if args.save_vae_training_history:
                    # El diccionario `history_data` ya est√° completo. Solo lo guardamos.
                    joblib.dump(history_data, fold_output_dir / f"vae_train_history_fold_{fold_idx+1}.joblib")
                    try:
                        fig, ax1 = plt.subplots(figsize=(12, 6)) # Un poco m√°s ancho
                        
                        # Graficar componentes del entrenamiento
                        ax1.plot(history_data["train_loss"], label="Train Loss (Total)", color='blue', linewidth=2)
                        ax1.plot(history_data["train_recon"], label="Train Recon Loss", color='cyan', linestyle=':')
                        ax1.plot(history_data["train_kld"], label="Train KLD Loss", color='magenta', linestyle=':')
                        
                        # Graficar componentes de la validaci√≥n si existen
                        if vae_internal_val_loader and any(not np.isnan(x) for x in history_data["val_loss"]):
                            ax1.plot(history_data["val_loss"], label="Val Loss (Total)", color='orange', linewidth=2)
                            ax1.plot(history_data["val_recon"], label="Val Recon Loss", color='#ff9966', linestyle='--') # Naranja claro
                            ax1.plot(history_data["val_kld"], label="Val KLD Loss", color='#ff66b2', linestyle='--') # Rosa/Rojo claro
                        
                        ax1.set_xlabel("Epoch")
                        ax1.set_ylabel("Loss Value")
                        ax1.set_title(f"Fold {fold_idx+1} VAE Training History")
                        ax1.legend(loc='upper left')
                        ax1.grid(True, linestyle='--', alpha=0.6)
                        ax1.set_ylim(bottom=0) # La p√©rdida no deber√≠a ser negativa

                        # Eje secundario para Beta
                        ax2 = ax1.twinx()
                        ax2.plot(history_data["beta"], label="Beta", color='green', linestyle='-.', alpha=0.8)
                        ax2.set_ylabel("Beta Value", color='green')
                        ax2.tick_params(axis='y', labelcolor='green')
                        ax2.legend(loc='upper right')

                        fig.tight_layout() # Ajusta el layout para que no se superpongan las etiquetas
                        plt.savefig(fold_output_dir / f"vae_train_history_fold_{fold_idx+1}.png")
                        plt.close(fig)
                    except Exception as e:
                        logger.warning(f"  {fold_idx_str} No se pudo guardar la gr√°fica de historial VAE: {e}")

        all_folds_vae_history.append(history_data if args.save_vae_training_history else None)
        
        # --- Obtenci√≥n de Caracter√≠sticas Latentes ---
        clf_train_dev_df = cn_ad_df.iloc[train_dev_clf_idx_in_cn_ad_df].copy()
        global_indices_clf_train_dev_all = clf_train_dev_df['tensor_idx'].values
        y_clf_train_dev_all = clf_train_dev_df['label'].values
        log_group_distributions(clf_train_dev_df, strat_cols, "Pool Train/Dev (Clasificador)", fold_idx_str)

        # --- Obtenci√≥n de Caracter√≠sticas Latentes ---
        vae_fold_k.eval()
        with torch.no_grad():
            full_train_dev_tensor_norm = apply_normalization_params(
                current_global_tensor[global_indices_clf_train_dev_all],
                norm_params_fold_list
            )
            _, mu_train_dev, _, z_train_dev = vae_fold_k(
                torch.from_numpy(full_train_dev_tensor_norm).float().to(device)
            )

            X_np = mu_train_dev.cpu().numpy() if args.latent_features_type == 'mu' else z_train_dev.cpu().numpy()
            feature_names = [f"latent_{i}" for i in range(X_np.shape[1])]
            X_latent_train_dev = pd.DataFrame(X_np, columns=feature_names)

            # ---------- TEST ----------
            # ... (c√≥digo para obtener X_latent_test_final es el mismo) ...
            X_test_final_tensor_norm = apply_normalization_params(
                current_global_tensor[global_indices_clf_test_this_fold],
                norm_params_fold_list
            )
            if X_test_final_tensor_norm.shape[0] > 0:
                _, mu_test_final, _, z_test_final = vae_fold_k(
                    torch.from_numpy(X_test_final_tensor_norm).float().to(device)
                )
                X_np_test = mu_test_final.cpu().numpy() if args.latent_features_type == 'mu' else z_test_final.cpu().numpy()
                X_latent_test_final = pd.DataFrame(X_np_test, columns=feature_names)
            else:
                X_latent_test_final = pd.DataFrame(columns=feature_names)
            y_test_final = y_classifier_labels_cn_ad[test_clf_idx_in_cn_ad_df]


            # --- ‚¨áÔ∏è NUEVO: Combinar con Caracter√≠sticas de Metadatos ‚¨áÔ∏è ---
            if args.metadata_features:
                logger.info(f"  A√±adiendo metadatos al clasificador: {args.metadata_features}")
                
                # --- Preparar metadatos para el conjunto de TRAIN/DEV ---
                metadata_train_dev = clf_train_dev_df[args.metadata_features].copy()
                
                # Codificar 'Sex' si est√° presente
                if 'Sex' in metadata_train_dev.columns:
                    metadata_train_dev['Sex'] = metadata_train_dev['Sex'].map({'M': 0, 'F': 1, 'f': 1, 'm': 0})

                # Manejar NaNs (imputaci√≥n simple usando la media/mediana del TRAIN set)
                imputation_values = {}
                for col in metadata_train_dev.columns:
                    if metadata_train_dev[col].isnull().any():
                        # Usamos la media para columnas num√©ricas con m√°s de 2 valores √∫nicos
                        if pd.api.types.is_numeric_dtype(metadata_train_dev[col]) and metadata_train_dev[col].nunique() > 2:
                            imputation_values[col] = metadata_train_dev[col].mean()
                        else: # Usamos la moda para categ√≥ricas/binarias
                            imputation_values[col] = metadata_train_dev[col].mode()[0]
                        
                        logger.warning(f"  Columna '{col}' tiene NaNs. Imputando con valor (train): {imputation_values[col]:.2f}")
                        metadata_train_dev[col].fillna(imputation_values[col], inplace=True)

                # Concatenar: crucial resetear los √≠ndices para una alineaci√≥n correcta
                X_latent_train_dev.reset_index(drop=True, inplace=True)
                metadata_train_dev.reset_index(drop=True, inplace=True)
                X_train_dev_combined = pd.concat([X_latent_train_dev, metadata_train_dev], axis=1)
                
                # Reemplazamos el DataFrame original
                X_latent_train_dev = X_train_dev_combined
                logger.info(f"  Forma final del set de entrenamiento del clasificador: {X_latent_train_dev.shape}")


                # --- Preparar metadatos para el conjunto de TEST ---
                if not X_latent_test_final.empty:
                    clf_test_df = cn_ad_df.iloc[test_clf_idx_in_cn_ad_df]
                    metadata_test = clf_test_df[args.metadata_features].copy()

                    # Codificar 'Sex' si est√° presente
                    if 'Sex' in metadata_test.columns:
                         metadata_test['Sex'] = metadata_test['Sex'].map({'M': 0, 'F': 1, 'f': 1, 'm': 0})
                    
                    # Aplicar la MISMA imputaci√≥n aprendida del set de TRAIN
                    for col, val in imputation_values.items():
                        metadata_test[col].fillna(val, inplace=True)

                    # Concatenar
                    X_latent_test_final.reset_index(drop=True, inplace=True)
                    metadata_test.reset_index(drop=True, inplace=True)
                    X_test_final_combined = pd.concat([X_latent_test_final, metadata_test], axis=1)
                    
                    # Reemplazar el DataFrame original
                    X_latent_test_final = X_test_final_combined



        # --- BUCLE DE CLASIFICADOR REFACTORIZADO ---
        for current_classifier_type in args.classifier_types:
            logger.info(f"    --- Entrenando Clasificador: {current_classifier_type} ---")
            
            try:
                # Obtenemos el pipeline completo y el grid de par√°metros
                full_pipeline, param_distributions, n_iter_search = get_classifier_and_grid(
                    classifier_type=current_classifier_type,
                    seed=args.seed,
                    balance=args.classifier_use_class_weight,
                    use_smote=(args.use_smote and not args.no_smote),
                    tune_sampler_params=args.tune_sampler_params,
                    mlp_hidden_layers=args.mlp_classifier_hidden_layers,
                    calibrate=args.classifier_calibrate
                )
            except (ImportError, ValueError) as e:
                logger.error(f"Error al obtener pipeline para {current_classifier_type}: {e}. Saltando.")
                continue

            # La divisi√≥n interna de CV se encargar√° de aplicar los pasos de forma correcta
            inner_cv_for_hp_tune = StratifiedKFold(n_splits=args.inner_folds, shuffle=True, random_state=args.seed + fold_idx + 30)

            if args.clf_no_tune:
                # üöÄ Sin b√∫squeda de HP: entrenar con defaults del pipeline
                final_clf_model = full_pipeline
                final_clf_model.fit(X_latent_train_dev.to_numpy(), y_clf_train_dev_all)
                best_params_clf = {"_no_tune": True}
                logger.info(f"      (sin tuning) {current_classifier_type} entrenado con hiperpar√°metros por defecto.")
            else:
                # --- B√∫squeda con Optuna (respetando l√≠mites v√≠a CLI) ---
                sampler = optuna.samplers.TPESampler(seed=args.seed)
                pruner = MedianPruner(n_warmup_steps=15, n_min_trials=5) if args.use_optuna_pruner else None
                if pruner:
                    logger.info("      Usando Optuna MedianPruner para acelerar la b√∫squeda.")
                study = optuna.create_study(direction="maximize", sampler=sampler, pruner=pruner)
                for k, v in param_distributions.items():
                    if not isinstance(v, optuna.distributions.BaseDistribution):
                        raise TypeError(f"{k} no es una distribuci√≥n de Optuna ({type(v)})")
                effective_n_trials = min(n_iter_search, args.optuna_max_trials)
                timeout_seconds = int(args.optuna_timeout)
                optuna_search = OptunaSearchCV(
                   estimator=full_pipeline,
                    param_distributions=param_distributions,
                    study=study,
                    cv=inner_cv_for_hp_tune,
                    scoring=args.gridsearch_scoring,
                    n_trials=effective_n_trials,
                    refit=True,
                    n_jobs=args.n_jobs_gridsearch,
                    timeout=timeout_seconds,
                    random_state=args.seed
                )
                optuna_search.fit(X_latent_train_dev.to_numpy(), y_clf_train_dev_all)
                best_params_clf = optuna_search.best_params_
                final_clf_model = optuna_search.best_estimator_
                logger.info(f"      Mejores HPs para {current_classifier_type}: {best_params_clf}")
                logger.info(f"      Modelo final (pipeline) para {current_classifier_type} listo.")

            # --- EVALUACI√ìN Y GUARDADO (SIMPLIFICADO) ---
            fold_results_clf = {
                'fold': fold_idx + 1, 
                'actual_classifier_type': current_classifier_type, 
                'best_clf_params': best_params_clf
            }

            if X_latent_test_final.shape[0] > 0:

                # --- ANTES (Con bloque especial para XGBoost) ---
                # if current_classifier_type == "xgb":
                #     # 1) Escalado solo si procede
                #     scaler_step = final_clf_model.named_steps.get("scaler", None)
                #     if scaler_step not in [None, "passthrough"]:
                #         X_test_arr = scaler_step.transform(X_latent_test_final)
                #     else:
                #         X_test_arr = X_latent_test_final.values
                #
                #     # 2) Predicci√≥n GPU nativa de XGBoost
                #     X_test_gpu = cp.asarray(X_test_arr, order="C")
                #     booster = final_clf_model.named_steps["model"].get_booster()
                #     booster.set_param({"predictor": "gpu_predictor"})
                #     y_pred_proba = cp.asnumpy(booster.inplace_predict(X_test_gpu))
                #     y_pred = (y_pred_proba >= 0.5).astype(int)
                #
                # else:
                #     # Para rf, gb, svm, etc. deja que el pipeline haga todo
                #     y_pred_proba = final_clf_model.predict_proba(X_latent_test_final)[:, 1]
                #     y_pred = final_clf_model.predict(X_latent_test_final)

                # --- SOLUCI√ìN (Unificada y Correcta) ---
                # Deja que el pipeline haga todo para TODOS los modelos.
                # XGBoost usar√° la GPU internamente porque fue inicializado con device="cuda".
                y_pred_proba = final_clf_model.predict_proba(X_latent_test_final)[:, 1]
                y_pred = final_clf_model.predict(X_latent_test_final)

                # M√©tricas
                fold_results_clf.update({
                    'auc': roc_auc_score(y_test_final, y_pred_proba),
                    'pr_auc': average_precision_score(y_test_final, y_pred_proba),
                    'accuracy': accuracy_score(y_test_final, y_pred),
                    'balanced_accuracy': balanced_accuracy_score(y_test_final, y_pred),
                    'sensitivity': recall_score(y_test_final, y_pred, pos_label=1, zero_division=0),
                    'specificity': recall_score(y_test_final, y_pred, pos_label=0, zero_division=0),
                    'f1_score': f1_score(y_test_final, y_pred, pos_label=1, zero_division=0)
                })
            else:
                for m in ['auc','pr_auc','accuracy','balanced_accuracy','sensitivity','specificity','f1_score']:
                    fold_results_clf[m] = np.nan

            logger.info(f"      Resultados Fold {fold_idx+1} ({current_classifier_type}): AUC={fold_results_clf.get('auc',np.nan):.4f}, Bal.Acc={fold_results_clf.get('balanced_accuracy',np.nan):.4f}")
            
            if args.save_fold_artefacts:
                # Guardamos el pipeline completo
                joblib.dump(final_clf_model, fold_output_dir / f"classifier_{current_classifier_type}_pipeline_fold_{fold_idx+1}.joblib")
                logger.info(f"      Pipeline completo de {current_classifier_type} del fold {fold_idx+1} guardado.")
            
            all_folds_metrics.append(fold_results_clf)
        
        del vae_fold_k, optimizer_vae, vae_train_loader, vae_internal_val_loader, scheduler_vae, best_model_state_dict
        gc.collect()
        if device.type == 'cuda': torch.cuda.empty_cache()
        logger.info(f"  {fold_idx_str} completado en {time.time() - fold_start_time:.2f} segundos.")

    # ... (La secci√≥n final de resumen y guardado de resultados no cambia) ...
    if all_folds_metrics:
        metrics_df = pd.DataFrame(all_folds_metrics)
        
        for clf_type_iterated in args.classifier_types:
            metrics_df_clf = metrics_df[metrics_df['actual_classifier_type'] == clf_type_iterated]
            if not metrics_df_clf.empty:
                logger.info(f"\n--- Resumen de Rendimiento para Clasificador: {clf_type_iterated} (Promedio sobre Folds Externos) ---")
                for metric in ['auc', 'pr_auc', 'accuracy', 'balanced_accuracy', 'sensitivity', 'specificity', 'f1_score']:
                    if metric in metrics_df_clf.columns and metrics_df_clf[metric].notna().any():
                        mean_val = metrics_df_clf[metric].mean()
                        std_val = metrics_df_clf[metric].std()
                        logger.info(f"{metric.capitalize():<20}: {mean_val:.4f} +/- {std_val:.4f}")
        
        main_clf_type_for_fname = args.classifier_types[0] if args.classifier_types else "genericclf"
        fname_suffix = (f"{main_clf_type_for_fname}_vae{args.decoder_type}{args.num_conv_layers_encoder}l_"
                        f"ld{args.latent_dim}_beta{args.beta_vae}_norm{args.norm_mode}_"
                        f"ch{num_input_channels_for_vae}{'sel' if args.channels_to_use else 'all'}_"
                        f"intFC{args.intermediate_fc_dim_vae}_drop{args.dropout_rate_vae}_"
                        f"ln{1 if args.use_layernorm_vae_fc else 0}_outer{args.outer_folds}x{args.repeated_outer_folds_n_repeats if args.repeated_outer_folds_n_repeats > 1 else 1}_"
                        f"score{args.gridsearch_scoring}")
        
        results_csv_path = output_base_dir / f"all_folds_metrics_MULTI_{fname_suffix}.csv"
        metrics_df.to_csv(results_csv_path, index=False)
        logger.info(f"Resultados detallados de todos los clasificadores guardados en: {results_csv_path}")

        summary_txt_path = output_base_dir / f"summary_metrics_MULTI_{fname_suffix}.txt"
        with open(summary_txt_path, 'w') as f:
            f.write(f"Run Arguments:\n{vars(args)}\n\n")
            f.write(f"Git Commit Hash: {args.git_hash}\n\n")
            for clf_type_iterated in args.classifier_types:
                metrics_df_clf = metrics_df[metrics_df['actual_classifier_type'] == clf_type_iterated]
                if not metrics_df_clf.empty:
                    f.write(f"--- Metrics Summary for Classifier: {clf_type_iterated} ---\n")
                    for metric in ['auc', 'pr_auc', 'accuracy', 'balanced_accuracy', 'sensitivity', 'specificity', 'f1_score']:
                        if metric in metrics_df_clf.columns and metrics_df_clf[metric].notna().any():
                            f.write(f"{metric.capitalize():<20}: {metrics_df_clf[metric].mean():.4f} +/- {metrics_df_clf[metric].std():.4f}\n")
                    f.write("\nFull Metrics DataFrame Description:\n")
                    f.write(metrics_df_clf.describe().to_string())
                    f.write("\n\n")
        logger.info(f"Sumario estad√≠stico de m√©tricas (por clasificador) guardado en: {summary_txt_path}")

        if args.save_vae_training_history and all_folds_vae_history:
             joblib.dump(all_folds_vae_history, output_base_dir / f"all_folds_vae_training_history_{fname_suffix}.joblib")
        if all_folds_clf_predictions: 
             joblib.dump(all_folds_clf_predictions, output_base_dir / f"all_folds_clf_predictions_MULTI_{fname_suffix}.joblib")
        return metrics_df
    else:
        logger.warning("No se pudieron calcular m√©tricas para ning√∫n fold.")
        return None

# --- Main Execution Block ---
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Pipeline VAE+Clasificador para AD/CN (v1.8.0)",
                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    
    # ... (Los argumentos no cambian, pero se a√±ade uno nuevo) ...
    group_data = parser.add_argument_group('Data and Paths')
    group_data.add_argument("--global_tensor_path", type=str, required=True, help="Ruta al archivo .npz del tensor global.")
    group_data.add_argument("--metadata_path", type=str, required=True, help="Ruta al archivo CSV de metadatos.")
    group_data.add_argument("--output_dir", type=str, default="./vae_clf_output_v1.7.0", help="Directorio para guardar resultados.")
    #group_data.add_argument("--channels_to_use", type=str, nargs='*', default=None, help="Lista de nombres o √≠ndices de canales a usar.")
    group_data.add_argument(
                                "--roi_order_path",
                                type=str,
                                required=True,
                                help="Fichero .npy/.txt/.csv con los 131 nombres de ROIs en el mismo orden \
                                    que las matrices de conectividad."
                            )
    # L√≠nea 1063
    group_data.add_argument("--channels_to_use", type=int, nargs='*', default=None, help="Lista de nombres o √≠ndices de canales a usar.")
    group_cv = parser.add_argument_group('Cross-validation')
    group_cv.add_argument("--outer_folds", type=int, default=5, help="N√∫mero de folds para CV externa del clasificador.")
    group_cv.add_argument("--repeated_outer_folds_n_repeats", type=int, default=1, help="N√∫mero de repeticiones para RepeatedStratifiedKFold.")
    group_cv.add_argument("--inner_folds", type=int, default=5, help="Folds para CV interna (RandomizedSearchCV).") 
    group_cv.add_argument("--classifier_stratify_cols", type=str, nargs='*', default=['Sex'], help="Columnas adicionales para estratificaci√≥n del clasificador.")
    group_cv.add_argument("--classifier_hp_tune_ratio", type=float, default=0.25, help="Proporci√≥n de datos de train/dev para ajuste de HP.")

    group_vae = parser.add_argument_group('VAE Model and Training')
    group_vae.add_argument("--num_conv_layers_encoder", type=int, default=4, choices=[3, 4], help="Capas convolucionales en encoder VAE.") 
    group_vae.add_argument("--decoder_type", type=str, default="convtranspose", choices=["upsample_conv", "convtranspose"], help="Tipo de decoder para VAE.") 
    group_vae.add_argument("--latent_dim", type=int, default=128, help="Dimensi√≥n del espacio latente VAE. (Recomendado: 128-256)")
    group_vae.add_argument("--lr_vae", type=float, default=1e-4, help="Tasa de aprendizaje VAE.")
    group_vae.add_argument("--epochs_vae", type=int, default=800, help="√âpocas m√°ximas para VAE.")
    group_vae.add_argument("--batch_size", type=int, default=32, help="Tama√±o del batch.")
    group_vae.add_argument("--beta_vae", type=float, default=1.0, help="Peso KLD (beta_max para annealing).")
    group_vae.add_argument("--cyclical_beta_n_cycles", type=int, default=4, help="Ciclos para annealing de beta.")
    group_vae.add_argument("--cyclical_beta_ratio_increase", type=float, default=0.4, help="Proporci√≥n de ciclo para aumentar beta. (Recomendado: 0.4)")
    group_vae.add_argument("--weight_decay_vae", type=float, default=1e-5, help="Decaimiento de peso (L2 reg) para VAE.")
    group_vae.add_argument("--vae_final_activation", type=str, default="linear", choices=["sigmoid", "tanh", "linear"], help="Activaci√≥n final del decoder VAE.")
    group_vae.add_argument("--intermediate_fc_dim_vae", type=str, default="quarter", help="Dimensi√≥n FC intermedia en VAE ('0', 'half', 'quarter', o entero).")
    group_vae.add_argument("--dropout_rate_vae", type=float, default=0.2, help="Tasa de dropout en VAE.")
    group_vae.add_argument("--use_layernorm_vae_fc", action='store_true', help="Usar LayerNorm en capas FC del VAE.")
    group_vae.add_argument("--vae_val_split_ratio", type=float, default=0.2, help="Proporci√≥n para validaci√≥n VAE.")
    group_vae.add_argument("--early_stopping_patience_vae", type=int, default=20, help="Paciencia early stopping VAE. (Recomendado: 15-20)")
    group_vae.add_argument("--lr_scheduler_patience_vae", type=int, default=15, help="Paciencia para el scheduler ReduceLROnPlateau del VAE.")
    # ‚ñº‚ñº‚ñº NUEVOS ARGUMENTOS ‚ñº‚ñº‚ñº
    group_vae.add_argument("--lr_scheduler_type", type=str, default="plateau", choices=["plateau", "cosine_warm"], help="Tipo de scheduler para el VAE.")
    group_vae.add_argument("--lr_scheduler_T0", type=int, default=50, help="√âpocas para el primer reinicio en CosineAnnealingWarmRestarts.")
    group_vae.add_argument("--lr_scheduler_eta_min", type=float, default=1e-7, help="Tasa de aprendizaje m√≠nima para CosineAnnealingWarmRestarts.")
    # ‚ñ≤‚ñ≤‚ñ≤ FIN NUEVOS ARGUMENTOS ‚ñ≤‚ñ≤‚ñ≤


    group_clf = parser.add_argument_group('Classifier')
    
    clf_choices = get_available_classifiers()
    group_clf.add_argument(
        "--classifier_types", nargs="+", default=["rf", "svm", "gb"],
        choices=clf_choices,
        help=f"Tipos de clasificadores a entrenar. Disponibles: {', '.join(clf_choices)}"
    )
    group_clf.add_argument("--clf_no_tune", action="store_true",
                           help="Entrena el clasificador con hiperpar√°metros por defecto (sin OptunaSearchCV). MUY r√°pido.")
    group_clf.add_argument("--optuna_max_trials", type=int, default=80,
                           help="M√°ximo de trials por b√∫squeda (antes hardcodeado a 80).")
    group_clf.add_argument("--optuna_timeout", type=int, default=1800,
                           help="Timeout en segundos por b√∫squeda (antes hardcodeado a 1800).")
    group_clf.add_argument("--use_optuna_pruner", action="store_true", 
                        help="Usar MedianPruner de Optuna para acelerar la b√∫squeda de HPs.")
    group_clf.add_argument("--latent_features_type", type=str, default="mu", choices=["mu", "z"], help="Usar 'mu' o 'z' como features latentes.")
    group_clf.add_argument("--gridsearch_scoring", type=str, default="balanced_accuracy", help="M√©trica para RandomizedSearchCV.")
    
    group_clf.add_argument("--classifier_use_class_weight", action="store_true", help="Usar class_weight='balanced' en clasificadores que lo soporten.")
    group_clf.add_argument("--classifier_calibrate", action="store_true", help="Aplicar calibraci√≥n de probabilidad a los clasificadores (CalibratedClassifierCV).")
    group_clf.add_argument("--use_smote", action="store_true", help="Usar SMOTE en el pipeline. (Recomendado activar)")
    group_clf.add_argument("--tune_sampler_params", action="store_true", help="Incluir hiperpar√°metros de SMOTE en la b√∫squeda de RandomizedSearch.")

    group_clf.add_argument("--mlp_classifier_hidden_layers", type=str, default="64,16", help="Capas ocultas para el clasificador MLP.")
    group_clf.add_argument(
        "--metadata_features", nargs="*", default=None,
        help="Lista de columnas de metadatos para a√±adir como features al clasificador (ej: Age Sex Years_of_Education)."
    )
    
    group_general = parser.add_argument_group('General and Saving Settings')
    group_general.add_argument("--norm_mode", type=str, default="zscore_offdiag", choices=["zscore_offdiag", "minmax_offdiag"], help="Modo de normalizaci√≥n inter-canal.")
    group_general.add_argument("--seed", type=int, default=42, help="Semilla para reproducibilidad.")
    group_general.add_argument("--num_workers", type=int, default=4, help="Workers para DataLoader.")
    group_general.add_argument("--n_jobs_gridsearch", type=int, default=4, help="Jobs para RandomizedSearchCV.")
    group_general.add_argument("--log_interval_epochs_vae", type=int, default=5, help="Intervalo de √©pocas para loguear VAE.")
    group_general.add_argument("--save_fold_artefacts", action='store_true', help="Guardar pipeline de clasificador de cada fold.")
    group_general.add_argument("--no_smote", action='store_true',
                               help="Fuerza desactivar SMOTE, aunque --use_smote est√© activado.")
    group_general.add_argument("--save_vae_training_history", action='store_true', help="Guardar historial de entrenamiento del VAE (loss, beta) por fold.")
    group_general.add_argument("--fast_ablation", action='store_true',
                               help="Preset r√°pido: VAE corto + logreg sin tuning; pensado para ranking de canales.")


    args = parser.parse_args()

    # ... (La l√≥gica de validaci√≥n de argumentos y configuraci√≥n de semilla no cambia) ...
    if isinstance(args.intermediate_fc_dim_vae, str) and args.intermediate_fc_dim_vae.lower() not in ["0", "half", "quarter"]:
        try:
            args.intermediate_fc_dim_vae = int(args.intermediate_fc_dim_vae)
        except ValueError:
            logger.error(f"Valor inv√°lido para intermediate_fc_dim_vae: {args.intermediate_fc_dim_vae}. Abortando.")
            exit(1)
    
    if not (0 <= args.vae_val_split_ratio < 1): 
        if args.vae_val_split_ratio != 0:
            logger.warning(f"vae_val_split_ratio ({args.vae_val_split_ratio}) inv√°lido. Se usar√° 0.")
        args.vae_val_split_ratio = 0
    
    if args.vae_val_split_ratio == 0:
        logger.info("Sin validaci√≥n VAE, early stopping y LR scheduler para VAE deshabilitados.")
        args.early_stopping_patience_vae = 0 
        args.lr_scheduler_patience_vae = 0

    # Aplicar preset --fast si existe el argumento (aunque no se use directamente)
    if getattr(args, 'fast_ablation', False):
        logger.info("[FAST PRESET DETECTADO EN serentipia9]")
        args.classifier_types = ["logreg"]
        args.clf_no_tune = True
        args.use_smote = False
        args.epochs_vae = min(getattr(args, 'epochs_vae', 300), 100)
        args.early_stopping_patience_vae = min(getattr(args, 'early_stopping_patience_vae', 30), 10)
        args.latent_dim = min(getattr(args, 'latent_dim', 512), 128)
        args.batch_size = max(getattr(args, 'batch_size', 64), 128)
        args.beta_vae = max(getattr(args, 'beta_vae', 1.0), 1.5)
        args.n_jobs_gridsearch = 1
        if args.no_smote:
            args.use_smote = False
    
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(args.seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    
    Path(args.output_dir).mkdir(parents=True, exist_ok=True)

    # --- INICIO NUEVO BLOQUE ---
    roi_order_path = Path(args.roi_order_path)
    if not roi_order_path.exists():
        logger.critical(f"Archivo de orden de ROIs no encontrado: {roi_order_path}")
        exit(1)

    if roi_order_path.suffix == ".npy":
        roi_order = np.load(roi_order_path).astype(str).tolist()
    else:                       # .txt o .csv con un ROI por l√≠nea/columna
        roi_order = pd.read_csv(roi_order_path, header=None).iloc[:, 0].astype(str).tolist()

    if len(roi_order) != 131:
        logger.critical(f"Se esperaban 131 ROIs y se leyeron {len(roi_order)}. Aborta.")
        exit(1)

    # guardamos copia inmutable para todo el experimento
    joblib.dump(roi_order, Path(args.output_dir) / "roi_order_131.joblib")
    logger.info(f"Lista de 131 ROIs guardada en: {args.output_dir}/roi_order_131.joblib")
    # --- FIN NUEVO BLOQUE ---

    try:
        git_hash = subprocess.check_output(['git', 'rev-parse', 'HEAD']).decode('ascii').strip()
        args.git_hash = git_hash
    except Exception:
        args.git_hash = "N/A"
    logger.info(f"Git commit hash: {args.git_hash}")

    logger.info("--- Configuraci√≥n de la Ejecuci√≥n (v1.7.0) ---")
    for arg, value in sorted(vars(args).items()):
        logger.info(f"{arg}: {value}")
    logger.info("------------------------------------")

    global_tensor_data, metadata_df_full = load_data(Path(args.global_tensor_path), Path(args.metadata_path))

    if global_tensor_data is not None and metadata_df_full is not None:
        pipeline_start_time = time.time()
        train_and_evaluate_pipeline(global_tensor_data, metadata_df_full, args)
        logger.info(f"Pipeline completo en {time.time() - pipeline_start_time:.2f} segundos.")
    else:
        logger.critical("No se pudieron cargar los datos. Abortando.")

    logger.info("--- Consideraciones Finales ---")
    logger.info(f"Normalizaci√≥n: '{args.norm_mode}'. Activaci√≥n VAE: '{args.vae_final_activation}'. Asegurar compatibilidad.")