{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbd77a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.6.0.99\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMRegressor(device=&#x27;gpu&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMRegressor</label><div class=\"sk-toggleable__content\"><pre>LGBMRegressor(device=&#x27;gpu&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMRegressor(device='gpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "print(lgb.__version__)\n",
    "'4.6.0.99'  # or whatever commit you built\n",
    "lgb.LGBMRegressor(device='gpu')  # should now work without complaint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a943e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿GPU visible?: True\n",
      "[INFO] [cuML] ➜ no disponible\n",
      "torch.cuda.is_available -> True\n",
      "torch.version.cuda      -> 12.1\n",
      "cupy GPU visible        -> True\n",
      "¿GPU visible?: True\n",
      "[INFO] Usando dispositivo: cuda\n",
      "[INFO] Lista de 131 ROIs guardada en: ./resultados_13_paper/roi_order_131.joblib\n",
      "[INFO] Git commit hash: 823b285a218a77026258f78daf4f9a9354ffc060\n",
      "[INFO] --- Configuración de la Ejecución (v1.7.0) ---\n",
      "[INFO] batch_size: 64\n",
      "[INFO] beta_vae: 4.6\n",
      "[INFO] channels_to_use: [1, 2, 5]\n",
      "[INFO] classifier_calibrate: True\n",
      "[INFO] classifier_hp_tune_ratio: 0.25\n",
      "[INFO] classifier_stratify_cols: ['Sex']\n",
      "[INFO] classifier_types: ['xgb', 'svm', 'logreg', 'gb', 'mlp']\n",
      "[INFO] classifier_use_class_weight: True\n",
      "[INFO] cyclical_beta_n_cycles: 4\n",
      "[INFO] cyclical_beta_ratio_increase: 0.4\n",
      "[INFO] decoder_type: convtranspose\n",
      "[INFO] dropout_rate_vae: 0.2\n",
      "[INFO] early_stopping_patience_vae: 30\n",
      "[INFO] epochs_vae: 300\n",
      "[INFO] git_hash: 823b285a218a77026258f78daf4f9a9354ffc060\n",
      "[INFO] global_tensor_path: /home/diego/Escritorio/limpio/AAL3_dynamicROIs_fmri_tensor_NeuroEnhanced_v6.5.17_AAL3_131ROIs_OMST_GCE_Signed_GrangerLag1_ChNorm_ROIreorderedYeo17_ParallelTuned/GLOBAL_TENSOR_from_AAL3_dynamicROIs_fmri_tensor_NeuroEnhanced_v6.5.17_AAL3_131ROIs_OMST_GCE_Signed_GrangerLag1_ChNorm_ROIreorderedYeo17_ParallelTuned.npz\n",
      "[INFO] gridsearch_scoring: roc_auc\n",
      "[INFO] inner_folds: 5\n",
      "[INFO] intermediate_fc_dim_vae: quarter\n",
      "[INFO] latent_dim: 512\n",
      "[INFO] latent_features_type: mu\n",
      "[INFO] log_interval_epochs_vae: 5\n",
      "[INFO] lr_scheduler_T0: 30\n",
      "[INFO] lr_scheduler_eta_min: 5e-07\n",
      "[INFO] lr_scheduler_patience_vae: 15\n",
      "[INFO] lr_scheduler_type: cosine_warm\n",
      "[INFO] lr_vae: 0.0001\n",
      "[INFO] metadata_features: ['Age', 'Sex']\n",
      "[INFO] metadata_path: /home/diego/Escritorio/limpio/SubjectsData_AAL3_procesado.csv\n",
      "[INFO] mlp_classifier_hidden_layers: 64,16\n",
      "[INFO] n_jobs_gridsearch: 8\n",
      "[INFO] norm_mode: zscore_offdiag\n",
      "[INFO] num_conv_layers_encoder: 4\n",
      "[INFO] num_workers: 4\n",
      "[INFO] outer_folds: 5\n",
      "[INFO] output_dir: ./resultados_13_paper\n",
      "[INFO] repeated_outer_folds_n_repeats: 1\n",
      "[INFO] roi_order_path: /home/diego/Escritorio/limpio/roi_order_131.npy\n",
      "[INFO] save_fold_artefacts: True\n",
      "[INFO] save_vae_training_history: True\n",
      "[INFO] seed: 42\n",
      "[INFO] tune_sampler_params: False\n",
      "[INFO] use_layernorm_vae_fc: False\n",
      "[INFO] use_optuna_pruner: True\n",
      "[INFO] use_smote: True\n",
      "[INFO] vae_final_activation: tanh\n",
      "[INFO] vae_val_split_ratio: 0.2\n",
      "[INFO] weight_decay_vae: 1e-05\n",
      "[INFO] ------------------------------------\n",
      "[INFO] Cargando tensor global desde: /home/diego/Escritorio/limpio/AAL3_dynamicROIs_fmri_tensor_NeuroEnhanced_v6.5.17_AAL3_131ROIs_OMST_GCE_Signed_GrangerLag1_ChNorm_ROIreorderedYeo17_ParallelTuned/GLOBAL_TENSOR_from_AAL3_dynamicROIs_fmri_tensor_NeuroEnhanced_v6.5.17_AAL3_131ROIs_OMST_GCE_Signed_GrangerLag1_ChNorm_ROIreorderedYeo17_ParallelTuned.npz\n",
      "[INFO] Tensor global cargado. Forma: (431, 7, 131, 131)\n",
      "[INFO] Cargando metadatos desde: /home/diego/Escritorio/limpio/SubjectsData_AAL3_procesado.csv\n",
      "[INFO] Metadatos cargados. Forma: (434, 33)\n",
      "[INFO] Usando canales seleccionados (índices): [1, 2, 5]\n",
      "[INFO] Nombres de canales seleccionados: ['Pearson_Full_FisherZ_Signed', 'MI_KNN_Symmetric', 'DistanceCorr']\n",
      "[INFO] Estratificando folds del CLASIFICADOR por: ['ResearchGroup_Mapped', 'Sex']\n",
      "[INFO] Sujetos CN/AD para clasificación: 184. CN: 89, AD: 95\n",
      "[INFO] Usando CV externa: StratifiedKFold con 5 iteraciones totales.\n",
      "[INFO] --- Iniciando Fold 1/5 ---\n",
      "[INFO] Fold 1/5 Test Set (Clasificador) (N=37):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 19 (51.4%)\n",
      "      CN: 18 (48.6%)\n",
      "    Sex:\n",
      "      F: 19 (51.4%)\n",
      "      M: 18 (48.6%)\n",
      "[INFO] Fold 1/5 Pool Entrenamiento VAE (N=394):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 76 (19.3%)\n",
      "      CN: 71 (18.0%)\n",
      "      MCI: 247 (62.7%)\n",
      "    Sex:\n",
      "      F: 187 (47.5%)\n",
      "      M: 207 (52.5%)\n",
      "    Age_Group:\n",
      "      0: 104 (26.4%)\n",
      "      1: 102 (25.9%)\n",
      "      2: 91 (23.1%)\n",
      "      3: 97 (24.6%)\n",
      "[INFO]   Fold 1/5 VAE val split será estratificado por ['ResearchGroup_Mapped', 'Sex', 'Age_Group'].\n",
      "[INFO] Fold 1/5 Actual Train Set (VAE) (N=315):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 62 (19.7%)\n",
      "      CN: 56 (17.8%)\n",
      "      MCI: 197 (62.5%)\n",
      "    Sex:\n",
      "      F: 149 (47.3%)\n",
      "      M: 166 (52.7%)\n",
      "    Age_Group:\n",
      "      0: 84 (26.7%)\n",
      "      1: 81 (25.7%)\n",
      "      2: 73 (23.2%)\n",
      "      3: 77 (24.4%)\n",
      "[INFO] Fold 1/5 Internal Val Set (VAE) (N=79):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 14 (17.7%)\n",
      "      CN: 15 (19.0%)\n",
      "      MCI: 50 (63.3%)\n",
      "    Sex:\n",
      "      F: 38 (48.1%)\n",
      "      M: 41 (51.9%)\n",
      "    Age_Group:\n",
      "      0: 20 (25.3%)\n",
      "      1: 21 (26.6%)\n",
      "      2: 18 (22.8%)\n",
      "      3: 20 (25.3%)\n",
      "[INFO]   Fold 1/5 Sujetos VAE actual train: 315, VAE internal val: 79\n",
      "[INFO] Aplicando normalización inter-canal (modo: zscore_offdiag) sobre 3 canales seleccionados.\n",
      "[INFO] Parámetros de normalización se calcularán usando 315 sujetos de entrenamiento.\n",
      "[INFO] Canal 'Pearson_Full_FisherZ_Signed': Off-diag zscore_offdiag (train_params: mean=-0.046, std=0.777)\n",
      "[INFO] Canal 'MI_KNN_Symmetric': Off-diag zscore_offdiag (train_params: mean=0.059, std=0.815)\n",
      "[INFO] Canal 'DistanceCorr': Off-diag zscore_offdiag (train_params: mean=-0.397, std=1.627)\n",
      "[INFO]   Fold 1/5 Usando dispositivo: cuda\n",
      "[INFO]   Fold 1/5 Usando scheduler: CosineAnnealingWarmRestarts (T_0=30)\n",
      "[INFO]   Fold 1/5 Entrenando VAE (Decoder: convtranspose, Encoder Layers: 4)...\n",
      "[INFO]   Fold 1/5 VAE E5/300, TrL: 63636.71 (R: 63529.25, KLD: 175.21), Beta: 0.613, LR: 9.38e-05, ValL: 62546.02 (R: 62496.61, KLD: 80.55)\n",
      "[INFO]   Fold 1/5 VAE E10/300, TrL: 59977.91 (R: 59592.09, KLD: 279.57), Beta: 1.380, LR: 7.60e-05, ValL: 57788.76 (R: 57241.38, KLD: 396.65)\n",
      "[INFO]   Fold 1/5 VAE E15/300, TrL: 54408.46 (R: 53495.13, KLD: 425.47), Beta: 2.147, LR: 5.13e-05, ValL: 53497.49 (R: 52402.00, KLD: 510.32)\n",
      "[INFO]   Fold 1/5 VAE E20/300, TrL: 52394.33 (R: 51547.20, KLD: 290.78), Beta: 2.913, LR: 2.63e-05, ValL: 51485.38 (R: 50712.87, KLD: 265.16)\n",
      "[INFO]   Fold 1/5 VAE E25/300, TrL: 51790.76 (R: 50844.04, KLD: 257.26), Beta: 3.680, LR: 7.70e-06, ValL: 50963.04 (R: 50105.93, KLD: 232.91)\n",
      "[INFO]   Fold 1/5 VAE E30/300, TrL: 51893.70 (R: 50775.49, KLD: 251.47), Beta: 4.447, LR: 5.11e-07, ValL: 51024.61 (R: 50014.46, KLD: 227.17)\n",
      "[INFO]   Fold 1/5 VAE E35/300, TrL: 49155.37 (R: 48303.83, KLD: 185.12), Beta: 4.600, LR: 9.38e-05, ValL: 47397.89 (R: 46670.16, KLD: 158.20)\n",
      "[INFO]   Fold 1/5 VAE E40/300, TrL: 46575.50 (R: 45837.99, KLD: 160.33), Beta: 4.600, LR: 7.60e-05, ValL: 44384.35 (R: 43749.73, KLD: 137.96)\n",
      "[INFO]   Fold 1/5 VAE E45/300, TrL: 44483.92 (R: 43752.13, KLD: 159.08), Beta: 4.600, LR: 5.13e-05, ValL: 42307.70 (R: 41712.07, KLD: 129.49)\n",
      "[INFO]   Fold 1/5 VAE E50/300, TrL: 43436.63 (R: 42734.72, KLD: 152.59), Beta: 4.600, LR: 2.63e-05, ValL: 41521.41 (R: 40936.82, KLD: 127.08)\n",
      "[INFO]   Fold 1/5 VAE E55/300, TrL: 43021.91 (R: 42343.31, KLD: 147.52), Beta: 4.600, LR: 7.70e-06, ValL: 41116.95 (R: 40577.61, KLD: 117.25)\n",
      "[INFO]   Fold 1/5 VAE E60/300, TrL: 42909.96 (R: 42251.35, KLD: 143.18), Beta: 4.600, LR: 5.11e-07, ValL: 40979.08 (R: 40468.50, KLD: 111.00)\n",
      "[INFO]   Fold 1/5 VAE E65/300, TrL: 41582.32 (R: 40883.24, KLD: 151.97), Beta: 4.600, LR: 9.38e-05, ValL: 39597.01 (R: 39052.58, KLD: 118.36)\n",
      "[INFO]   Fold 1/5 VAE E70/300, TrL: 40461.78 (R: 39790.41, KLD: 145.95), Beta: 4.600, LR: 7.60e-05, ValL: 38725.94 (R: 38195.84, KLD: 115.24)\n",
      "[INFO]   Fold 1/5 VAE E75/300, TrL: 39663.12 (R: 39018.74, KLD: 140.08), Beta: 4.600, LR: 5.13e-05, ValL: 38096.70 (R: 37577.61, KLD: 112.85)\n",
      "[INFO]   Fold 1/5 VAE E80/300, TrL: 38573.62 (R: 38464.86, KLD: 177.33), Beta: 0.613, LR: 2.63e-05, ValL: 37350.84 (R: 37261.17, KLD: 146.19)\n",
      "[INFO]   Fold 1/5 VAE E85/300, TrL: 38665.49 (R: 38408.69, KLD: 186.09), Beta: 1.380, LR: 7.70e-06, ValL: 37246.62 (R: 37036.40, KLD: 152.33)\n",
      "[INFO]   Fold 1/5 VAE E90/300, TrL: 38685.11 (R: 38287.26, KLD: 185.33), Beta: 2.147, LR: 5.11e-07, ValL: 37341.02 (R: 37007.73, KLD: 155.26)\n",
      "[INFO]   Fold 1/5 VAE E95/300, TrL: 38204.55 (R: 37701.56, KLD: 172.65), Beta: 2.913, LR: 9.38e-05, ValL: 36866.20 (R: 36434.93, KLD: 148.03)\n",
      "[INFO]   Fold 1/5 VAE E100/300, TrL: 37736.07 (R: 37098.96, KLD: 173.13), Beta: 3.680, LR: 7.60e-05, ValL: 36501.55 (R: 35985.73, KLD: 140.17)\n",
      "[INFO]   Fold 1/5 VAE E105/300, TrL: 37450.10 (R: 36727.29, KLD: 162.55), Beta: 4.447, LR: 5.13e-05, ValL: 36300.08 (R: 35698.95, KLD: 135.19)\n",
      "[INFO]   Fold 1/5 VAE E110/300, TrL: 37330.38 (R: 36642.11, KLD: 149.62), Beta: 4.600, LR: 2.63e-05, ValL: 36076.38 (R: 35536.40, KLD: 117.39)\n",
      "[INFO]   Fold 1/5 VAE E115/300, TrL: 37150.76 (R: 36482.76, KLD: 145.22), Beta: 4.600, LR: 7.70e-06, ValL: 35948.14 (R: 35425.04, KLD: 113.72)\n",
      "[INFO]   Fold 1/5 VAE E120/300, TrL: 37060.03 (R: 36406.72, KLD: 142.02), Beta: 4.600, LR: 5.11e-07, ValL: 35875.41 (R: 35364.35, KLD: 111.10)\n",
      "[INFO]   Fold 1/5 VAE E125/300, TrL: 36820.28 (R: 36169.53, KLD: 141.47), Beta: 4.600, LR: 9.38e-05, ValL: 35602.39 (R: 35060.48, KLD: 117.80)\n",
      "[INFO]   Fold 1/5 VAE E130/300, TrL: 36371.77 (R: 35675.50, KLD: 151.36), Beta: 4.600, LR: 7.60e-05, ValL: 35271.08 (R: 34716.80, KLD: 120.49)\n",
      "[INFO]   Fold 1/5 VAE E135/300, TrL: 36093.94 (R: 35446.62, KLD: 140.72), Beta: 4.600, LR: 5.13e-05, ValL: 35063.93 (R: 34530.31, KLD: 116.00)\n",
      "[INFO]   Fold 1/5 VAE E140/300, TrL: 35918.80 (R: 35261.17, KLD: 142.96), Beta: 4.600, LR: 2.63e-05, ValL: 34836.09 (R: 34322.79, KLD: 111.59)\n",
      "[INFO]   Fold 1/5 VAE E145/300, TrL: 35924.59 (R: 35278.09, KLD: 140.54), Beta: 4.600, LR: 7.70e-06, ValL: 34807.62 (R: 34294.30, KLD: 111.59)\n",
      "[INFO]   Fold 1/5 VAE E150/300, TrL: 35730.84 (R: 35085.54, KLD: 140.28), Beta: 4.600, LR: 5.11e-07, ValL: 34828.22 (R: 34320.05, KLD: 110.47)\n",
      "[INFO]   Fold 1/5 VAE E155/300, TrL: 34864.72 (R: 34732.35, KLD: 215.81), Beta: 0.613, LR: 9.38e-05, ValL: 34168.31 (R: 34050.27, KLD: 192.45)\n",
      "[INFO]   Fold 1/5 VAE E160/300, TrL: 34718.85 (R: 34389.48, KLD: 238.67), Beta: 1.380, LR: 7.60e-05, ValL: 33921.46 (R: 33634.84, KLD: 207.69)\n",
      "[INFO]   Fold 1/5 VAE E165/300, TrL: 34620.59 (R: 34139.56, KLD: 224.08), Beta: 2.147, LR: 5.13e-05, ValL: 33869.41 (R: 33451.28, KLD: 194.78)\n",
      "[INFO]   Fold 1/5 VAE E170/300, TrL: 34660.73 (R: 34050.22, KLD: 209.56), Beta: 2.913, LR: 2.63e-05, ValL: 33952.53 (R: 33429.29, KLD: 179.60)\n",
      "[INFO]   Fold 1/5 VAE E175/300, TrL: 34676.41 (R: 33945.26, KLD: 198.68), Beta: 3.680, LR: 7.70e-06, ValL: 33974.15 (R: 33347.48, KLD: 170.29)\n",
      "[INFO]   Fold 1/5 VAE E180/300, TrL: 34652.34 (R: 33789.03, KLD: 194.15), Beta: 4.447, LR: 5.11e-07, ValL: 34127.44 (R: 33386.01, KLD: 166.74)\n",
      "[INFO]   Fold 1/5 VAE E185/300, TrL: 34700.42 (R: 33998.19, KLD: 152.66), Beta: 4.600, LR: 9.38e-05, ValL: 33989.34 (R: 33381.43, KLD: 132.15)\n",
      "[INFO]   Fold 1/5 VAE E190/300, TrL: 34501.76 (R: 33767.69, KLD: 159.58), Beta: 4.600, LR: 7.60e-05, ValL: 33770.93 (R: 33160.65, KLD: 132.67)\n",
      "[INFO]   Fold 1/5 VAE E195/300, TrL: 34259.13 (R: 33547.64, KLD: 154.67), Beta: 4.600, LR: 5.13e-05, ValL: 33634.53 (R: 33059.02, KLD: 125.11)\n",
      "[INFO]   Fold 1/5 VAE E200/300, TrL: 34164.65 (R: 33467.71, KLD: 151.51), Beta: 4.600, LR: 2.63e-05, ValL: 33484.09 (R: 32926.12, KLD: 121.30)\n",
      "[INFO]   Fold 1/5 VAE E205/300, TrL: 34083.85 (R: 33390.47, KLD: 150.73), Beta: 4.600, LR: 7.70e-06, ValL: 33485.34 (R: 32938.22, KLD: 118.94)\n",
      "[INFO]   Fold 1/5 VAE E210/300, TrL: 33981.44 (R: 33295.41, KLD: 149.14), Beta: 4.600, LR: 5.11e-07, ValL: 33447.87 (R: 32908.31, KLD: 117.30)\n",
      "[INFO]   Fold 1/5 VAE E215/300, TrL: 33916.58 (R: 33208.28, KLD: 153.98), Beta: 4.600, LR: 9.38e-05, ValL: 33318.63 (R: 32735.03, KLD: 126.87)\n",
      "[INFO]   Fold 1/5 VAE E220/300, TrL: 33779.00 (R: 33064.20, KLD: 155.39), Beta: 4.600, LR: 7.60e-05, ValL: 33235.70 (R: 32663.76, KLD: 124.33)\n",
      "[INFO]   Fold 1/5 VAE E225/300, TrL: 33579.03 (R: 32876.25, KLD: 152.78), Beta: 4.600, LR: 5.13e-05, ValL: 33116.41 (R: 32556.40, KLD: 121.74)\n",
      "[INFO]   Fold 1/5 VAE E230/300, TrL: 32891.16 (R: 32782.16, KLD: 177.71), Beta: 0.613, LR: 2.63e-05, ValL: 32467.02 (R: 32374.37, KLD: 151.07)\n",
      "[INFO]   Fold 1/5 VAE E235/300, TrL: 32754.70 (R: 32490.88, KLD: 191.17), Beta: 1.380, LR: 7.70e-06, ValL: 32549.83 (R: 32327.93, KLD: 160.79)\n",
      "[INFO]   Fold 1/5 VAE E240/300, TrL: 32930.28 (R: 32518.57, KLD: 191.79), Beta: 2.147, LR: 5.11e-07, ValL: 32709.06 (R: 32358.80, KLD: 163.16)\n",
      "[INFO]   Fold 1/5 VAE E245/300, TrL: 33029.68 (R: 32466.27, KLD: 193.39), Beta: 2.913, LR: 9.38e-05, ValL: 32700.77 (R: 32220.13, KLD: 164.98)\n",
      "[INFO]   Fold 1/5 VAE E250/300, TrL: 33076.54 (R: 32395.86, KLD: 184.97), Beta: 3.680, LR: 7.60e-05, ValL: 32690.10 (R: 32119.40, KLD: 155.08)\n",
      "[INFO]   Fold 1/5 VAE E255/300, TrL: 32954.81 (R: 32181.08, KLD: 174.00), Beta: 4.447, LR: 5.13e-05, ValL: 32710.28 (R: 32076.46, KLD: 142.54)\n",
      "[INFO]   Fold 1/5 Early stopping VAE en epoch 258. Mejor val_loss: 32443.8422 (época 228)\n",
      "[INFO]   Fold 1/5 VAE final model loaded (best val_loss: 32443.8422).\n",
      "[INFO]   Fold 1/5 Modelo VAE guardado en: resultados_13_paper/fold_1/vae_model_fold_1.pt\n",
      "[INFO] Fold 1/5 Pool Train/Dev (Clasificador) (N=147):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 76 (51.7%)\n",
      "      CN: 71 (48.3%)\n",
      "    Sex:\n",
      "      F: 73 (49.7%)\n",
      "      M: 74 (50.3%)\n",
      "[INFO]   Añadiendo metadatos al clasificador: ['Age', 'Sex']\n",
      "[INFO]   Forma final del set de entrenamiento del clasificador: (147, 514)\n",
      "[INFO]     --- Entrenando Clasificador: xgb ---\n",
      "[XGBoost] ➜  Se usará GPU (device=cuda)\n",
      "[INFO] [SMOTE] ➜ aplicado sólo dentro de folds (imblearn Pipeline).\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para xgb: {'model__gamma': 1.8507947978181774, 'model__n_estimators': 690, 'model__learning_rate': 0.028973835994436578, 'model__max_depth': 9, 'model__subsample': 0.6878433875921696, 'model__colsample_bytree': 0.9277460864941757, 'model__min_child_weight': 6.993566026322865}\n",
      "[INFO]       Modelo final (pipeline) para xgb listo.\n",
      "[INFO]       Resultados Fold 1 (xgb): AUC=0.8304, Bal.Acc=0.7573\n",
      "[INFO]       Pipeline completo de xgb del fold 1 guardado.\n",
      "[INFO]     --- Entrenando Clasificador: svm ---\n",
      "[INFO] [SMOTE] ➜ aplicado sólo dentro de folds (imblearn Pipeline).\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para svm: {'model__estimator__C': 4850.351159570844, 'model__estimator__gamma': 8.790806359417369e-07, 'model__estimator__kernel': 'rbf'}\n",
      "[INFO]       Modelo final (pipeline) para svm listo.\n",
      "[INFO]       Resultados Fold 1 (svm): AUC=0.8743, Bal.Acc=0.7047\n",
      "[INFO]       Pipeline completo de svm del fold 1 guardado.\n",
      "[INFO]     --- Entrenando Clasificador: logreg ---\n",
      "[INFO] [SMOTE] ➜ aplicado sólo dentro de folds (imblearn Pipeline).\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para logreg: {'model__C': 0.0065535995618746755}\n",
      "[INFO]       Modelo final (pipeline) para logreg listo.\n",
      "[INFO]       Resultados Fold 1 (logreg): AUC=0.8684, Bal.Acc=0.7558\n",
      "[INFO]       Pipeline completo de logreg del fold 1 guardado.\n",
      "[INFO]     --- Entrenando Clasificador: gb ---\n",
      "[LightGBM] ⚠ No se pudo comprobar la GPU, usando CPU\n",
      "[INFO] [SMOTE] ➜ aplicado sólo dentro de folds (imblearn Pipeline).\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para gb: {'model__estimator__max_depth': 3, 'model__estimator__num_leaves': 453, 'model__estimator__bagging_fraction': 0.9680035004063042, 'model__estimator__feature_fraction': 0.9936646688886926, 'model__estimator__bagging_freq': 2, 'model__estimator__learning_rate': 0.0017181159229251067, 'model__estimator__n_estimators': 344, 'model__estimator__min_child_samples': 9, 'model__estimator__min_child_weight': 0.005540618687063777, 'model__estimator__min_split_gain': 0.7472338591036499, 'model__estimator__reg_alpha': 0.0035088495369000956, 'model__estimator__reg_lambda': 0.21376246136544008}\n",
      "[INFO]       Modelo final (pipeline) para gb listo.\n",
      "[INFO]       Resultados Fold 1 (gb): AUC=0.7675, Bal.Acc=0.7047\n",
      "[INFO]       Pipeline completo de gb del fold 1 guardado.\n",
      "[INFO]     --- Entrenando Clasificador: mlp ---\n",
      "[INFO] [SMOTE] ➜ aplicado sólo dentro de folds (imblearn Pipeline).\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para mlp: {'model__alpha': 0.08787736784266664, 'model__learning_rate_init': 0.004357962434975709}\n",
      "[INFO]       Modelo final (pipeline) para mlp listo.\n",
      "[INFO]       Resultados Fold 1 (mlp): AUC=0.8772, Bal.Acc=0.7807\n",
      "[INFO]       Pipeline completo de mlp del fold 1 guardado.\n",
      "[INFO]   Fold 1/5 completado en 434.30 segundos.\n",
      "[INFO] --- Iniciando Fold 2/5 ---\n",
      "[INFO] Fold 2/5 Test Set (Clasificador) (N=37):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 19 (51.4%)\n",
      "      CN: 18 (48.6%)\n",
      "    Sex:\n",
      "      F: 19 (51.4%)\n",
      "      M: 18 (48.6%)\n",
      "[INFO] Fold 2/5 Pool Entrenamiento VAE (N=394):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 76 (19.3%)\n",
      "      CN: 71 (18.0%)\n",
      "      MCI: 247 (62.7%)\n",
      "    Sex:\n",
      "      F: 187 (47.5%)\n",
      "      M: 207 (52.5%)\n",
      "    Age_Group:\n",
      "      0: 98 (24.9%)\n",
      "      1: 101 (25.6%)\n",
      "      2: 94 (23.9%)\n",
      "      3: 101 (25.6%)\n",
      "[INFO]   Fold 2/5 VAE val split será estratificado por ['ResearchGroup_Mapped', 'Sex', 'Age_Group'].\n",
      "[INFO] Fold 2/5 Actual Train Set (VAE) (N=315):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 61 (19.4%)\n",
      "      CN: 57 (18.1%)\n",
      "      MCI: 197 (62.5%)\n",
      "    Sex:\n",
      "      F: 150 (47.6%)\n",
      "      M: 165 (52.4%)\n",
      "    Age_Group:\n",
      "      0: 79 (25.1%)\n",
      "      1: 81 (25.7%)\n",
      "      2: 75 (23.8%)\n",
      "      3: 80 (25.4%)\n",
      "[INFO] Fold 2/5 Internal Val Set (VAE) (N=79):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 15 (19.0%)\n",
      "      CN: 14 (17.7%)\n",
      "      MCI: 50 (63.3%)\n",
      "    Sex:\n",
      "      F: 37 (46.8%)\n",
      "      M: 42 (53.2%)\n",
      "    Age_Group:\n",
      "      0: 19 (24.1%)\n",
      "      1: 20 (25.3%)\n",
      "      2: 19 (24.1%)\n",
      "      3: 21 (26.6%)\n",
      "[INFO]   Fold 2/5 Sujetos VAE actual train: 315, VAE internal val: 79\n",
      "[INFO] Aplicando normalización inter-canal (modo: zscore_offdiag) sobre 3 canales seleccionados.\n",
      "[INFO] Parámetros de normalización se calcularán usando 315 sujetos de entrenamiento.\n",
      "[INFO] Canal 'Pearson_Full_FisherZ_Signed': Off-diag zscore_offdiag (train_params: mean=-0.049, std=0.776)\n",
      "[INFO] Canal 'MI_KNN_Symmetric': Off-diag zscore_offdiag (train_params: mean=0.055, std=0.813)\n",
      "[INFO] Canal 'DistanceCorr': Off-diag zscore_offdiag (train_params: mean=-0.408, std=1.730)\n",
      "[INFO]   Fold 2/5 Usando dispositivo: cuda\n",
      "[INFO]   Fold 2/5 Usando scheduler: CosineAnnealingWarmRestarts (T_0=30)\n",
      "[INFO]   Fold 2/5 Entrenando VAE (Decoder: convtranspose, Encoder Layers: 4)...\n",
      "[INFO]   Fold 2/5 VAE E5/300, TrL: 63775.31 (R: 63668.20, KLD: 174.63), Beta: 0.613, LR: 9.38e-05, ValL: 57912.93 (R: 57864.20, KLD: 79.45)\n",
      "[INFO]   Fold 2/5 VAE E10/300, TrL: 60478.99 (R: 60118.98, KLD: 260.87), Beta: 1.380, LR: 7.60e-05, ValL: 53159.87 (R: 52578.26, KLD: 421.46)\n",
      "[INFO]   Fold 2/5 VAE E15/300, TrL: 54435.39 (R: 53487.92, KLD: 441.37), Beta: 2.147, LR: 5.13e-05, ValL: 49329.79 (R: 48041.55, KLD: 600.11)\n",
      "[INFO]   Fold 2/5 VAE E20/300, TrL: 52655.84 (R: 51799.57, KLD: 293.91), Beta: 2.913, LR: 2.63e-05, ValL: 47438.51 (R: 46572.56, KLD: 297.24)\n",
      "[INFO]   Fold 2/5 VAE E25/300, TrL: 52091.02 (R: 51157.48, KLD: 253.68), Beta: 3.680, LR: 7.70e-06, ValL: 47022.32 (R: 46085.24, KLD: 254.64)\n",
      "[INFO]   Fold 2/5 VAE E30/300, TrL: 52214.19 (R: 51116.08, KLD: 246.95), Beta: 4.447, LR: 5.11e-07, ValL: 47040.92 (R: 45937.35, KLD: 248.18)\n",
      "[INFO]   Fold 2/5 VAE E35/300, TrL: 49499.42 (R: 48690.37, KLD: 175.88), Beta: 4.600, LR: 9.38e-05, ValL: 44098.17 (R: 43234.41, KLD: 187.77)\n",
      "[INFO]   Fold 2/5 VAE E40/300, TrL: 47033.50 (R: 46353.19, KLD: 147.89), Beta: 4.600, LR: 7.60e-05, ValL: 41341.70 (R: 40695.93, KLD: 140.39)\n",
      "[INFO]   Fold 2/5 VAE E45/300, TrL: 45415.16 (R: 44799.92, KLD: 133.75), Beta: 4.600, LR: 5.13e-05, ValL: 39503.98 (R: 38946.94, KLD: 121.09)\n",
      "[INFO]   Fold 2/5 VAE E50/300, TrL: 44312.06 (R: 43709.11, KLD: 131.08), Beta: 4.600, LR: 2.63e-05, ValL: 38455.28 (R: 37924.42, KLD: 115.41)\n",
      "[INFO]   Fold 2/5 VAE E55/300, TrL: 43906.48 (R: 43320.16, KLD: 127.46), Beta: 4.600, LR: 7.70e-06, ValL: 38009.33 (R: 37498.62, KLD: 111.03)\n",
      "[INFO]   Fold 2/5 VAE E60/300, TrL: 43854.47 (R: 43274.85, KLD: 126.00), Beta: 4.600, LR: 5.11e-07, ValL: 37927.09 (R: 37423.72, KLD: 109.43)\n",
      "[INFO]   Fold 2/5 VAE E65/300, TrL: 42313.46 (R: 41732.42, KLD: 126.31), Beta: 4.600, LR: 9.38e-05, ValL: 36179.35 (R: 35632.99, KLD: 118.78)\n",
      "[INFO]   Fold 2/5 VAE E70/300, TrL: 41042.16 (R: 40455.88, KLD: 127.45), Beta: 4.600, LR: 7.60e-05, ValL: 34994.93 (R: 34457.55, KLD: 116.82)\n",
      "[INFO]   Fold 2/5 VAE E75/300, TrL: 40249.37 (R: 39638.62, KLD: 132.77), Beta: 4.600, LR: 5.13e-05, ValL: 34258.79 (R: 33725.99, KLD: 115.83)\n",
      "[INFO]   Fold 2/5 VAE E80/300, TrL: 39248.47 (R: 39145.61, KLD: 167.71), Beta: 0.613, LR: 2.63e-05, ValL: 33449.40 (R: 33348.78, KLD: 164.06)\n",
      "[INFO]   Fold 2/5 VAE E85/300, TrL: 39312.59 (R: 39065.70, KLD: 178.91), Beta: 1.380, LR: 7.70e-06, ValL: 33394.07 (R: 33157.75, KLD: 171.25)\n",
      "[INFO]   Fold 2/5 VAE E90/300, TrL: 39330.97 (R: 38945.00, KLD: 179.80), Beta: 2.147, LR: 5.11e-07, ValL: 33478.73 (R: 33108.48, KLD: 172.48)\n",
      "[INFO]   Fold 2/5 VAE E95/300, TrL: 38757.12 (R: 38288.33, KLD: 160.91), Beta: 2.913, LR: 9.38e-05, ValL: 32755.53 (R: 32331.79, KLD: 145.45)\n",
      "[INFO]   Fold 2/5 VAE E100/300, TrL: 38226.32 (R: 37631.56, KLD: 161.62), Beta: 3.680, LR: 7.60e-05, ValL: 32338.90 (R: 31804.26, KLD: 145.28)\n",
      "[INFO]   Fold 2/5 VAE E105/300, TrL: 37891.98 (R: 37248.44, KLD: 144.72), Beta: 4.447, LR: 5.13e-05, ValL: 32076.99 (R: 31474.97, KLD: 135.39)\n",
      "[INFO]   Fold 2/5 VAE E110/300, TrL: 37563.87 (R: 36929.39, KLD: 137.93), Beta: 4.600, LR: 2.63e-05, ValL: 31769.05 (R: 31244.36, KLD: 114.06)\n",
      "[INFO]   Fold 2/5 VAE E115/300, TrL: 37475.18 (R: 36841.47, KLD: 137.76), Beta: 4.600, LR: 7.70e-06, ValL: 31694.02 (R: 31176.56, KLD: 112.49)\n",
      "[INFO]   Fold 2/5 VAE E120/300, TrL: 37406.83 (R: 36784.05, KLD: 135.39), Beta: 4.600, LR: 5.11e-07, ValL: 31645.77 (R: 31142.84, KLD: 109.33)\n",
      "[INFO]   Fold 2/5 VAE E125/300, TrL: 37180.91 (R: 36516.76, KLD: 144.38), Beta: 4.600, LR: 9.38e-05, ValL: 31534.83 (R: 31008.60, KLD: 114.40)\n",
      "[INFO]   Fold 2/5 VAE E130/300, TrL: 36672.56 (R: 36026.21, KLD: 140.51), Beta: 4.600, LR: 7.60e-05, ValL: 31078.76 (R: 30542.98, KLD: 116.47)\n",
      "[INFO]   Fold 2/5 VAE E135/300, TrL: 36460.21 (R: 35818.38, KLD: 139.53), Beta: 4.600, LR: 5.13e-05, ValL: 30818.51 (R: 30274.02, KLD: 118.37)\n",
      "[INFO]   Fold 2/5 VAE E140/300, TrL: 36258.87 (R: 35637.25, KLD: 135.14), Beta: 4.600, LR: 2.63e-05, ValL: 30651.19 (R: 30132.33, KLD: 112.80)\n",
      "[INFO]   Fold 2/5 VAE E145/300, TrL: 36088.35 (R: 35467.52, KLD: 134.96), Beta: 4.600, LR: 7.70e-06, ValL: 30581.59 (R: 30070.43, KLD: 111.12)\n",
      "[INFO]   Fold 2/5 VAE E150/300, TrL: 36089.30 (R: 35465.33, KLD: 135.64), Beta: 4.600, LR: 5.11e-07, ValL: 30562.55 (R: 30052.31, KLD: 110.92)\n",
      "[INFO]   Fold 2/5 VAE E155/300, TrL: 35136.91 (R: 35003.71, KLD: 217.16), Beta: 0.613, LR: 9.38e-05, ValL: 29765.38 (R: 29640.89, KLD: 202.98)\n",
      "[INFO]   Fold 2/5 VAE E160/300, TrL: 34933.21 (R: 34601.48, KLD: 240.38), Beta: 1.380, LR: 7.60e-05, ValL: 29694.26 (R: 29389.58, KLD: 220.78)\n",
      "[INFO]   Fold 2/5 VAE E165/300, TrL: 34986.41 (R: 34519.50, KLD: 217.50), Beta: 2.147, LR: 5.13e-05, ValL: 29726.28 (R: 29297.81, KLD: 199.60)\n",
      "[INFO]   Fold 2/5 VAE E170/300, TrL: 34940.70 (R: 34338.63, KLD: 206.66), Beta: 2.913, LR: 2.63e-05, ValL: 29683.54 (R: 29142.53, KLD: 185.70)\n",
      "[INFO]   Fold 2/5 VAE E175/300, TrL: 34945.99 (R: 34230.47, KLD: 194.44), Beta: 3.680, LR: 7.70e-06, ValL: 29825.77 (R: 29202.28, KLD: 169.43)\n",
      "[INFO]   Fold 2/5 VAE E180/300, TrL: 35133.29 (R: 34282.80, KLD: 191.26), Beta: 4.447, LR: 5.11e-07, ValL: 29968.57 (R: 29224.17, KLD: 167.41)\n",
      "[INFO]   Fold 2/5 VAE E185/300, TrL: 35027.70 (R: 34313.45, KLD: 155.27), Beta: 4.600, LR: 9.38e-05, ValL: 29770.20 (R: 29177.38, KLD: 128.87)\n",
      "[INFO]   Fold 2/5 VAE E190/300, TrL: 34754.93 (R: 34046.08, KLD: 154.10), Beta: 4.600, LR: 7.60e-05, ValL: 29505.85 (R: 28932.36, KLD: 124.67)\n",
      "[INFO]   Fold 2/5 VAE E195/300, TrL: 34514.89 (R: 33803.22, KLD: 154.71), Beta: 4.600, LR: 5.13e-05, ValL: 29413.48 (R: 28842.11, KLD: 124.21)\n",
      "[INFO]   Fold 2/5 VAE E200/300, TrL: 34375.43 (R: 33681.09, KLD: 150.94), Beta: 4.600, LR: 2.63e-05, ValL: 29337.33 (R: 28776.03, KLD: 122.02)\n",
      "[INFO]   Fold 2/5 VAE E205/300, TrL: 34313.27 (R: 33633.53, KLD: 147.77), Beta: 4.600, LR: 7.70e-06, ValL: 29261.54 (R: 28709.65, KLD: 119.98)\n",
      "[INFO]   Fold 2/5 VAE E210/300, TrL: 34309.52 (R: 33629.62, KLD: 147.80), Beta: 4.600, LR: 5.11e-07, ValL: 29291.58 (R: 28746.89, KLD: 118.41)\n",
      "[INFO]   Fold 2/5 VAE E215/300, TrL: 34181.81 (R: 33475.26, KLD: 153.60), Beta: 4.600, LR: 9.38e-05, ValL: 29226.59 (R: 28660.71, KLD: 123.02)\n",
      "[INFO]   Fold 2/5 VAE E220/300, TrL: 34132.00 (R: 33438.51, KLD: 150.76), Beta: 4.600, LR: 7.60e-05, ValL: 29088.46 (R: 28509.02, KLD: 125.96)\n",
      "[INFO]   Fold 2/5 VAE E225/300, TrL: 33864.46 (R: 33159.86, KLD: 153.17), Beta: 4.600, LR: 5.13e-05, ValL: 28919.79 (R: 28345.69, KLD: 124.80)\n",
      "[INFO]   Fold 2/5 VAE E230/300, TrL: 33049.12 (R: 32937.01, KLD: 182.80), Beta: 0.613, LR: 2.63e-05, ValL: 28321.30 (R: 28225.80, KLD: 155.71)\n",
      "[INFO]   Fold 2/5 VAE E235/300, TrL: 33120.26 (R: 32851.67, KLD: 194.63), Beta: 1.380, LR: 7.70e-06, ValL: 28442.44 (R: 28212.29, KLD: 166.77)\n",
      "[INFO]   Fold 2/5 VAE E240/300, TrL: 33251.63 (R: 32830.67, KLD: 196.10), Beta: 2.147, LR: 5.11e-07, ValL: 28580.84 (R: 28218.65, KLD: 168.72)\n",
      "[INFO]   Fold 2/5 VAE E245/300, TrL: 33309.29 (R: 32751.93, KLD: 191.31), Beta: 2.913, LR: 9.38e-05, ValL: 28527.37 (R: 28053.87, KLD: 162.53)\n",
      "[INFO]   Fold 2/5 VAE E250/300, TrL: 33397.25 (R: 32728.59, KLD: 181.70), Beta: 3.680, LR: 7.60e-05, ValL: 28489.58 (R: 27928.58, KLD: 152.45)\n",
      "[INFO]   Fold 2/5 VAE E255/300, TrL: 33406.34 (R: 32652.44, KLD: 169.54), Beta: 4.447, LR: 5.13e-05, ValL: 28596.71 (R: 27988.84, KLD: 136.70)\n",
      "[INFO]   Fold 2/5 Early stopping VAE en epoch 258. Mejor val_loss: 28251.1155 (época 228)\n",
      "[INFO]   Fold 2/5 VAE final model loaded (best val_loss: 28251.1155).\n",
      "[INFO]   Fold 2/5 Modelo VAE guardado en: resultados_13_paper/fold_2/vae_model_fold_2.pt\n",
      "[INFO] Fold 2/5 Pool Train/Dev (Clasificador) (N=147):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 76 (51.7%)\n",
      "      CN: 71 (48.3%)\n",
      "    Sex:\n",
      "      F: 73 (49.7%)\n",
      "      M: 74 (50.3%)\n",
      "[INFO]   Añadiendo metadatos al clasificador: ['Age', 'Sex']\n",
      "[INFO]   Forma final del set de entrenamiento del clasificador: (147, 514)\n",
      "[INFO]     --- Entrenando Clasificador: xgb ---\n",
      "[XGBoost] ➜  Se usará GPU (device=cuda)\n",
      "[INFO] [SMOTE] ➜ aplicado sólo dentro de folds (imblearn Pipeline).\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para xgb: {'model__gamma': 3.247461550023152, 'model__n_estimators': 1440, 'model__learning_rate': 0.010742673847170228, 'model__max_depth': 12, 'model__subsample': 0.42865730214850417, 'model__colsample_bytree': 0.6936502845738027, 'model__min_child_weight': 2.708167726248622}\n",
      "[INFO]       Modelo final (pipeline) para xgb listo.\n",
      "[INFO]       Resultados Fold 2 (xgb): AUC=0.7515, Bal.Acc=0.6769\n",
      "[INFO]       Pipeline completo de xgb del fold 2 guardado.\n",
      "[INFO]     --- Entrenando Clasificador: svm ---\n",
      "[INFO] [SMOTE] ➜ aplicado sólo dentro de folds (imblearn Pipeline).\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para svm: {'model__estimator__C': 408.2177272128284, 'model__estimator__gamma': 1.1015671302743262e-06, 'model__estimator__kernel': 'rbf'}\n",
      "[INFO]       Modelo final (pipeline) para svm listo.\n",
      "[INFO]       Resultados Fold 2 (svm): AUC=0.7763, Bal.Acc=0.6988\n",
      "[INFO]       Pipeline completo de svm del fold 2 guardado.\n",
      "[INFO]     --- Entrenando Clasificador: logreg ---\n",
      "[INFO] [SMOTE] ➜ aplicado sólo dentro de folds (imblearn Pipeline).\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para logreg: {'model__C': 0.005501285990449514}\n",
      "[INFO]       Modelo final (pipeline) para logreg listo.\n",
      "[INFO]       Resultados Fold 2 (logreg): AUC=0.8070, Bal.Acc=0.7032\n",
      "[INFO]       Pipeline completo de logreg del fold 2 guardado.\n",
      "[INFO]     --- Entrenando Clasificador: gb ---\n",
      "[LightGBM] ⚠ No se pudo comprobar la GPU, usando CPU\n",
      "[INFO] [SMOTE] ➜ aplicado sólo dentro de folds (imblearn Pipeline).\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para gb: {'model__estimator__max_depth': 10, 'model__estimator__num_leaves': 1023, 'model__estimator__bagging_fraction': 0.7748472020232293, 'model__estimator__feature_fraction': 0.6966722870559959, 'model__estimator__bagging_freq': 8, 'model__estimator__learning_rate': 0.005263392038364186, 'model__estimator__n_estimators': 693, 'model__estimator__min_child_samples': 25, 'model__estimator__min_child_weight': 1.5469672232228782, 'model__estimator__min_split_gain': 0.3585927985938787, 'model__estimator__reg_alpha': 0.013563759730682324, 'model__estimator__reg_lambda': 0.038269691860503996}\n",
      "[INFO]       Modelo final (pipeline) para gb listo.\n",
      "[INFO]       Resultados Fold 2 (gb): AUC=0.7266, Bal.Acc=0.6740\n",
      "[INFO]       Pipeline completo de gb del fold 2 guardado.\n",
      "[INFO]     --- Entrenando Clasificador: mlp ---\n",
      "[INFO] [SMOTE] ➜ aplicado sólo dentro de folds (imblearn Pipeline).\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para mlp: {'model__alpha': 0.09485888744940885, 'model__learning_rate_init': 0.007602330118135733}\n",
      "[INFO]       Modelo final (pipeline) para mlp listo.\n",
      "[INFO]       Resultados Fold 2 (mlp): AUC=0.8187, Bal.Acc=0.7281\n",
      "[INFO]       Pipeline completo de mlp del fold 2 guardado.\n",
      "[INFO]   Fold 2/5 completado en 553.50 segundos.\n",
      "[INFO] --- Iniciando Fold 3/5 ---\n",
      "[INFO] Fold 3/5 Test Set (Clasificador) (N=37):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 19 (51.4%)\n",
      "      CN: 18 (48.6%)\n",
      "    Sex:\n",
      "      F: 18 (48.6%)\n",
      "      M: 19 (51.4%)\n",
      "[INFO] Fold 3/5 Pool Entrenamiento VAE (N=394):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 76 (19.3%)\n",
      "      CN: 71 (18.0%)\n",
      "      MCI: 247 (62.7%)\n",
      "    Sex:\n",
      "      F: 188 (47.7%)\n",
      "      M: 206 (52.3%)\n",
      "    Age_Group:\n",
      "      0: 97 (24.6%)\n",
      "      1: 104 (26.4%)\n",
      "      2: 97 (24.6%)\n",
      "      3: 96 (24.4%)\n",
      "[INFO]   Fold 3/5 VAE val split será estratificado por ['ResearchGroup_Mapped', 'Sex', 'Age_Group'].\n",
      "[INFO] Fold 3/5 Actual Train Set (VAE) (N=315):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 60 (19.0%)\n",
      "      CN: 58 (18.4%)\n",
      "      MCI: 197 (62.5%)\n",
      "    Sex:\n",
      "      F: 150 (47.6%)\n",
      "      M: 165 (52.4%)\n",
      "    Age_Group:\n",
      "      0: 78 (24.8%)\n",
      "      1: 83 (26.3%)\n",
      "      2: 78 (24.8%)\n",
      "      3: 76 (24.1%)\n",
      "[INFO] Fold 3/5 Internal Val Set (VAE) (N=79):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 16 (20.3%)\n",
      "      CN: 13 (16.5%)\n",
      "      MCI: 50 (63.3%)\n",
      "    Sex:\n",
      "      F: 38 (48.1%)\n",
      "      M: 41 (51.9%)\n",
      "    Age_Group:\n",
      "      0: 19 (24.1%)\n",
      "      1: 21 (26.6%)\n",
      "      2: 19 (24.1%)\n",
      "      3: 20 (25.3%)\n",
      "[INFO]   Fold 3/5 Sujetos VAE actual train: 315, VAE internal val: 79\n",
      "[INFO] Aplicando normalización inter-canal (modo: zscore_offdiag) sobre 3 canales seleccionados.\n",
      "[INFO] Parámetros de normalización se calcularán usando 315 sujetos de entrenamiento.\n",
      "[INFO] Canal 'Pearson_Full_FisherZ_Signed': Off-diag zscore_offdiag (train_params: mean=-0.050, std=0.780)\n",
      "[INFO] Canal 'MI_KNN_Symmetric': Off-diag zscore_offdiag (train_params: mean=0.054, std=0.812)\n",
      "[INFO] Canal 'DistanceCorr': Off-diag zscore_offdiag (train_params: mean=-0.419, std=1.767)\n",
      "[INFO]   Fold 3/5 Usando dispositivo: cuda\n",
      "[INFO]   Fold 3/5 Usando scheduler: CosineAnnealingWarmRestarts (T_0=30)\n",
      "[INFO]   Fold 3/5 Entrenando VAE (Decoder: convtranspose, Encoder Layers: 4)...\n",
      "[INFO]   Fold 3/5 VAE E5/300, TrL: 63178.29 (R: 63071.28, KLD: 174.48), Beta: 0.613, LR: 9.38e-05, ValL: 55836.43 (R: 55786.70, KLD: 81.08)\n",
      "[INFO]   Fold 3/5 VAE E10/300, TrL: 59725.00 (R: 59350.97, KLD: 271.03), Beta: 1.380, LR: 7.60e-05, ValL: 51136.71 (R: 50495.06, KLD: 464.97)\n",
      "[INFO]   Fold 3/5 VAE E15/300, TrL: 54561.11 (R: 53673.48, KLD: 413.49), Beta: 2.147, LR: 5.13e-05, ValL: 48228.68 (R: 46814.23, KLD: 658.91)\n",
      "[INFO]   Fold 3/5 VAE E20/300, TrL: 52674.52 (R: 51841.57, KLD: 285.91), Beta: 2.913, LR: 2.63e-05, ValL: 46529.39 (R: 45538.65, KLD: 340.07)\n",
      "[INFO]   Fold 3/5 VAE E25/300, TrL: 52242.36 (R: 51339.91, KLD: 245.23), Beta: 3.680, LR: 7.70e-06, ValL: 46171.16 (R: 45075.43, KLD: 297.75)\n",
      "[INFO]   Fold 3/5 VAE E30/300, TrL: 52340.27 (R: 51266.85, KLD: 241.40), Beta: 4.447, LR: 5.11e-07, ValL: 46283.20 (R: 44980.01, KLD: 293.07)\n",
      "[INFO]   Fold 3/5 VAE E35/300, TrL: 49777.24 (R: 48983.77, KLD: 172.49), Beta: 4.600, LR: 9.38e-05, ValL: 43406.43 (R: 42434.31, KLD: 211.33)\n",
      "[INFO]   Fold 3/5 VAE E40/300, TrL: 47214.13 (R: 46540.93, KLD: 146.35), Beta: 4.600, LR: 7.60e-05, ValL: 40638.59 (R: 39791.62, KLD: 184.12)\n",
      "[INFO]   Fold 3/5 VAE E45/300, TrL: 45291.95 (R: 44660.78, KLD: 137.21), Beta: 4.600, LR: 5.13e-05, ValL: 38841.24 (R: 38098.20, KLD: 161.53)\n",
      "[INFO]   Fold 3/5 VAE E50/300, TrL: 44308.65 (R: 43715.71, KLD: 128.90), Beta: 4.600, LR: 2.63e-05, ValL: 37812.46 (R: 37161.28, KLD: 141.56)\n",
      "[INFO]   Fold 3/5 VAE E55/300, TrL: 43975.93 (R: 43385.25, KLD: 128.41), Beta: 4.600, LR: 7.70e-06, ValL: 37444.57 (R: 36808.91, KLD: 138.19)\n",
      "[INFO]   Fold 3/5 VAE E60/300, TrL: 43735.01 (R: 43162.13, KLD: 124.54), Beta: 4.600, LR: 5.11e-07, ValL: 37384.62 (R: 36770.55, KLD: 133.49)\n",
      "[INFO]   Fold 3/5 VAE E65/300, TrL: 42448.20 (R: 41869.52, KLD: 125.80), Beta: 4.600, LR: 9.38e-05, ValL: 35898.87 (R: 35246.54, KLD: 141.81)\n",
      "[INFO]   Fold 3/5 VAE E70/300, TrL: 41156.76 (R: 40558.23, KLD: 130.12), Beta: 4.600, LR: 7.60e-05, ValL: 34996.77 (R: 34278.91, KLD: 156.06)\n",
      "[INFO]   Fold 3/5 VAE E75/300, TrL: 40305.86 (R: 39737.15, KLD: 123.63), Beta: 4.600, LR: 5.13e-05, ValL: 34243.48 (R: 33624.07, KLD: 134.65)\n",
      "[INFO]   Fold 3/5 VAE E80/300, TrL: 39328.08 (R: 39226.52, KLD: 165.60), Beta: 0.613, LR: 2.63e-05, ValL: 33304.50 (R: 33183.78, KLD: 196.83)\n",
      "[INFO]   Fold 3/5 VAE E85/300, TrL: 39266.48 (R: 39017.88, KLD: 180.14), Beta: 1.380, LR: 7.70e-06, ValL: 33352.04 (R: 33047.45, KLD: 220.72)\n",
      "[INFO]   Fold 3/5 VAE E90/300, TrL: 39553.83 (R: 39172.39, KLD: 177.69), Beta: 2.147, LR: 5.11e-07, ValL: 33521.89 (R: 33050.82, KLD: 219.44)\n",
      "[INFO]   Fold 3/5 VAE E95/300, TrL: 38988.86 (R: 38485.55, KLD: 172.76), Beta: 2.913, LR: 9.38e-05, ValL: 32931.28 (R: 32332.13, KLD: 205.66)\n",
      "[INFO]   Fold 3/5 VAE E100/300, TrL: 38287.76 (R: 37705.47, KLD: 158.23), Beta: 3.680, LR: 7.60e-05, ValL: 32396.91 (R: 31773.97, KLD: 169.28)\n",
      "[INFO]   Fold 3/5 VAE E105/300, TrL: 37976.17 (R: 37325.53, KLD: 146.32), Beta: 4.447, LR: 5.13e-05, ValL: 32036.48 (R: 31363.38, KLD: 151.37)\n",
      "[INFO]   Fold 3/5 VAE E110/300, TrL: 37625.23 (R: 37000.06, KLD: 135.91), Beta: 4.600, LR: 2.63e-05, ValL: 31772.01 (R: 31140.67, KLD: 137.25)\n",
      "[INFO]   Fold 3/5 VAE E115/300, TrL: 37436.49 (R: 36823.07, KLD: 133.35), Beta: 4.600, LR: 7.70e-06, ValL: 31718.48 (R: 31108.93, KLD: 132.51)\n",
      "[INFO]   Fold 3/5 VAE E120/300, TrL: 37557.17 (R: 36942.95, KLD: 133.53), Beta: 4.600, LR: 5.11e-07, ValL: 31719.66 (R: 31113.89, KLD: 131.69)\n",
      "[INFO]   Fold 3/5 VAE E125/300, TrL: 37096.49 (R: 36470.67, KLD: 136.05), Beta: 4.600, LR: 9.38e-05, ValL: 31366.29 (R: 30737.13, KLD: 136.77)\n",
      "[INFO]   Fold 3/5 VAE E130/300, TrL: 36574.00 (R: 35941.75, KLD: 137.44), Beta: 4.600, LR: 7.60e-05, ValL: 30999.52 (R: 30365.47, KLD: 137.84)\n",
      "[INFO]   Fold 3/5 VAE E135/300, TrL: 36279.10 (R: 35645.78, KLD: 137.68), Beta: 4.600, LR: 5.13e-05, ValL: 30689.19 (R: 30087.32, KLD: 130.84)\n",
      "[INFO]   Fold 3/5 VAE E140/300, TrL: 36059.51 (R: 35452.79, KLD: 131.89), Beta: 4.600, LR: 2.63e-05, ValL: 30558.16 (R: 29994.37, KLD: 122.56)\n",
      "[INFO]   Fold 3/5 VAE E145/300, TrL: 36036.60 (R: 35422.09, KLD: 133.59), Beta: 4.600, LR: 7.70e-06, ValL: 30483.33 (R: 29914.22, KLD: 123.72)\n",
      "[INFO]   Fold 3/5 VAE E150/300, TrL: 36076.74 (R: 35463.39, KLD: 133.34), Beta: 4.600, LR: 5.11e-07, ValL: 30467.56 (R: 29912.33, KLD: 120.70)\n",
      "[INFO]   Fold 3/5 VAE E155/300, TrL: 35066.78 (R: 34928.63, KLD: 225.24), Beta: 0.613, LR: 9.38e-05, ValL: 29744.12 (R: 29595.13, KLD: 242.92)\n",
      "[INFO]   Fold 3/5 VAE E160/300, TrL: 34933.50 (R: 34599.75, KLD: 241.84), Beta: 1.380, LR: 7.60e-05, ValL: 29601.26 (R: 29249.71, KLD: 254.75)\n",
      "[INFO]   Fold 3/5 VAE E165/300, TrL: 34828.09 (R: 34349.53, KLD: 222.93), Beta: 2.147, LR: 5.13e-05, ValL: 29547.48 (R: 29051.03, KLD: 231.26)\n",
      "[INFO]   Fold 3/5 VAE E170/300, TrL: 34919.39 (R: 34317.44, KLD: 206.62), Beta: 2.913, LR: 2.63e-05, ValL: 29569.74 (R: 28967.46, KLD: 206.73)\n",
      "[INFO]   Fold 3/5 VAE E175/300, TrL: 34845.38 (R: 34134.02, KLD: 193.30), Beta: 3.680, LR: 7.70e-06, ValL: 29615.91 (R: 28934.02, KLD: 185.30)\n",
      "[INFO]   Fold 3/5 VAE E180/300, TrL: 35082.46 (R: 34246.96, KLD: 187.89), Beta: 4.447, LR: 5.11e-07, ValL: 29733.57 (R: 28933.12, KLD: 180.01)\n",
      "[INFO]   Fold 3/5 VAE E185/300, TrL: 34823.51 (R: 34128.26, KLD: 151.14), Beta: 4.600, LR: 9.38e-05, ValL: 29602.79 (R: 28946.41, KLD: 142.69)\n",
      "[INFO]   Fold 3/5 VAE E190/300, TrL: 34722.49 (R: 34026.78, KLD: 151.24), Beta: 4.600, LR: 7.60e-05, ValL: 29257.56 (R: 28633.91, KLD: 135.58)\n",
      "[INFO]   Fold 3/5 VAE E195/300, TrL: 34415.15 (R: 33728.83, KLD: 149.20), Beta: 4.600, LR: 5.13e-05, ValL: 29165.35 (R: 28565.02, KLD: 130.51)\n",
      "[INFO]   Fold 3/5 VAE E200/300, TrL: 34408.59 (R: 33717.96, KLD: 150.14), Beta: 4.600, LR: 2.63e-05, ValL: 29087.18 (R: 28476.67, KLD: 132.72)\n",
      "[INFO]   Fold 3/5 VAE E205/300, TrL: 34252.26 (R: 33585.93, KLD: 144.86), Beta: 4.600, LR: 7.70e-06, ValL: 29046.98 (R: 28462.54, KLD: 127.05)\n",
      "[INFO]   Fold 3/5 VAE E210/300, TrL: 34189.17 (R: 33521.51, KLD: 145.14), Beta: 4.600, LR: 5.11e-07, ValL: 29006.61 (R: 28421.15, KLD: 127.27)\n",
      "[INFO]   Fold 3/5 VAE E215/300, TrL: 34103.72 (R: 33389.38, KLD: 155.29), Beta: 4.600, LR: 9.38e-05, ValL: 28946.08 (R: 28317.42, KLD: 136.67)\n",
      "[INFO]   Fold 3/5 VAE E220/300, TrL: 33867.01 (R: 33150.37, KLD: 155.79), Beta: 4.600, LR: 7.60e-05, ValL: 28907.85 (R: 28231.23, KLD: 147.09)\n",
      "[INFO]   Fold 3/5 VAE E225/300, TrL: 33763.76 (R: 33053.80, KLD: 154.34), Beta: 4.600, LR: 5.13e-05, ValL: 28691.26 (R: 28053.27, KLD: 138.69)\n",
      "[INFO]   Fold 3/5 VAE E230/300, TrL: 33058.06 (R: 32947.39, KLD: 180.44), Beta: 0.613, LR: 2.63e-05, ValL: 28032.53 (R: 27931.20, KLD: 165.21)\n",
      "[INFO]   Fold 3/5 VAE E235/300, TrL: 33132.56 (R: 32864.61, KLD: 194.17), Beta: 1.380, LR: 7.70e-06, ValL: 28110.75 (R: 27863.21, KLD: 179.37)\n",
      "[INFO]   Fold 3/5 VAE E240/300, TrL: 33208.45 (R: 32794.30, KLD: 192.93), Beta: 2.147, LR: 5.11e-07, ValL: 28225.99 (R: 27842.59, KLD: 178.60)\n",
      "[INFO]   Fold 3/5 VAE E245/300, TrL: 33269.55 (R: 32720.76, KLD: 188.37), Beta: 2.913, LR: 9.38e-05, ValL: 28355.01 (R: 27876.15, KLD: 164.37)\n",
      "[INFO]   Fold 3/5 VAE E250/300, TrL: 33241.94 (R: 32571.18, KLD: 182.27), Beta: 3.680, LR: 7.60e-05, ValL: 28320.38 (R: 27737.55, KLD: 158.38)\n",
      "[INFO]   Fold 3/5 VAE E255/300, TrL: 33196.07 (R: 32449.26, KLD: 167.95), Beta: 4.447, LR: 5.13e-05, ValL: 28301.93 (R: 27663.20, KLD: 143.64)\n",
      "[INFO]   Fold 3/5 VAE E260/300, TrL: 33081.36 (R: 32336.34, KLD: 161.96), Beta: 4.600, LR: 2.63e-05, ValL: 28222.01 (R: 27596.38, KLD: 136.01)\n",
      "[INFO]   Fold 3/5 Early stopping VAE en epoch 261. Mejor val_loss: 28030.6184 (época 231)\n",
      "[INFO]   Fold 3/5 VAE final model loaded (best val_loss: 28030.6184).\n",
      "[INFO]   Fold 3/5 Modelo VAE guardado en: resultados_13_paper/fold_3/vae_model_fold_3.pt\n",
      "[INFO] Fold 3/5 Pool Train/Dev (Clasificador) (N=147):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 76 (51.7%)\n",
      "      CN: 71 (48.3%)\n",
      "    Sex:\n",
      "      F: 74 (50.3%)\n",
      "      M: 73 (49.7%)\n",
      "[INFO]   Añadiendo metadatos al clasificador: ['Age', 'Sex']\n",
      "[INFO]   Forma final del set de entrenamiento del clasificador: (147, 514)\n",
      "[INFO]     --- Entrenando Clasificador: xgb ---\n",
      "[XGBoost] ➜  Se usará GPU (device=cuda)\n",
      "[INFO] [SMOTE] ➜ aplicado sólo dentro de folds (imblearn Pipeline).\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para xgb: {'model__gamma': 0.02534668835144055, 'model__n_estimators': 505, 'model__learning_rate': 0.0211605143355234, 'model__max_depth': 10, 'model__subsample': 0.3372483344910435, 'model__colsample_bytree': 0.5297790309365784, 'model__min_child_weight': 0.5234034291667613}\n",
      "[INFO]       Modelo final (pipeline) para xgb listo.\n",
      "[INFO]       Resultados Fold 3 (xgb): AUC=0.8246, Bal.Acc=0.7281\n",
      "[INFO]       Pipeline completo de xgb del fold 3 guardado.\n",
      "[INFO]     --- Entrenando Clasificador: svm ---\n",
      "[INFO] [SMOTE] ➜ aplicado sólo dentro de folds (imblearn Pipeline).\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para svm: {'model__estimator__C': 5.601833002668573, 'model__estimator__gamma': 0.00023673254549090266, 'model__estimator__kernel': 'rbf'}\n",
      "[INFO]       Modelo final (pipeline) para svm listo.\n",
      "[INFO]       Resultados Fold 3 (svm): AUC=0.7851, Bal.Acc=0.7558\n",
      "[INFO]       Pipeline completo de svm del fold 3 guardado.\n",
      "[INFO]     --- Entrenando Clasificador: logreg ---\n",
      "[INFO] [SMOTE] ➜ aplicado sólo dentro de folds (imblearn Pipeline).\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para logreg: {'model__C': 0.002018282132712729}\n",
      "[INFO]       Modelo final (pipeline) para logreg listo.\n",
      "[INFO]       Resultados Fold 3 (logreg): AUC=0.8304, Bal.Acc=0.8085\n",
      "[INFO]       Pipeline completo de logreg del fold 3 guardado.\n",
      "[INFO]     --- Entrenando Clasificador: gb ---\n",
      "[LightGBM] ⚠ No se pudo comprobar la GPU, usando CPU\n",
      "[INFO] [SMOTE] ➜ aplicado sólo dentro de folds (imblearn Pipeline).\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para gb: {'model__estimator__max_depth': 3, 'model__estimator__num_leaves': 70, 'model__estimator__bagging_fraction': 0.7394278206241518, 'model__estimator__feature_fraction': 0.5894030748316029, 'model__estimator__bagging_freq': 2, 'model__estimator__learning_rate': 0.008019536023313383, 'model__estimator__n_estimators': 616, 'model__estimator__min_child_samples': 23, 'model__estimator__min_child_weight': 4.95363975128942, 'model__estimator__min_split_gain': 0.21798769857945038, 'model__estimator__reg_alpha': 0.004790705685494373, 'model__estimator__reg_lambda': 0.024845498994430403}\n",
      "[INFO]       Modelo final (pipeline) para gb listo.\n",
      "[INFO]       Resultados Fold 3 (gb): AUC=0.7792, Bal.Acc=0.7047\n",
      "[INFO]       Pipeline completo de gb del fold 3 guardado.\n",
      "[INFO]     --- Entrenando Clasificador: mlp ---\n",
      "[INFO] [SMOTE] ➜ aplicado sólo dentro de folds (imblearn Pipeline).\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para mlp: {'model__alpha': 0.00968466195627018, 'model__learning_rate_init': 0.004699693231927053}\n",
      "[INFO]       Modelo final (pipeline) para mlp listo.\n",
      "[INFO]       Resultados Fold 3 (mlp): AUC=0.7924, Bal.Acc=0.7018\n",
      "[INFO]       Pipeline completo de mlp del fold 3 guardado.\n",
      "[INFO]   Fold 3/5 completado en 603.76 segundos.\n",
      "[INFO] --- Iniciando Fold 4/5 ---\n",
      "[INFO] Fold 4/5 Test Set (Clasificador) (N=37):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 19 (51.4%)\n",
      "      CN: 18 (48.6%)\n",
      "    Sex:\n",
      "      F: 18 (48.6%)\n",
      "      M: 19 (51.4%)\n",
      "[INFO] Fold 4/5 Pool Entrenamiento VAE (N=394):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 76 (19.3%)\n",
      "      CN: 71 (18.0%)\n",
      "      MCI: 247 (62.7%)\n",
      "    Sex:\n",
      "      F: 188 (47.7%)\n",
      "      M: 206 (52.3%)\n",
      "    Age_Group:\n",
      "      0: 99 (25.1%)\n",
      "      1: 96 (24.4%)\n",
      "      2: 98 (24.9%)\n",
      "      3: 101 (25.6%)\n",
      "[INFO]   Fold 4/5 VAE val split será estratificado por ['ResearchGroup_Mapped', 'Sex', 'Age_Group'].\n",
      "[INFO] Fold 4/5 Actual Train Set (VAE) (N=315):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 62 (19.7%)\n",
      "      CN: 56 (17.8%)\n",
      "      MCI: 197 (62.5%)\n",
      "    Sex:\n",
      "      F: 150 (47.6%)\n",
      "      M: 165 (52.4%)\n",
      "    Age_Group:\n",
      "      0: 80 (25.4%)\n",
      "      1: 77 (24.4%)\n",
      "      2: 78 (24.8%)\n",
      "      3: 80 (25.4%)\n",
      "[INFO] Fold 4/5 Internal Val Set (VAE) (N=79):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 14 (17.7%)\n",
      "      CN: 15 (19.0%)\n",
      "      MCI: 50 (63.3%)\n",
      "    Sex:\n",
      "      F: 38 (48.1%)\n",
      "      M: 41 (51.9%)\n",
      "    Age_Group:\n",
      "      0: 19 (24.1%)\n",
      "      1: 19 (24.1%)\n",
      "      2: 20 (25.3%)\n",
      "      3: 21 (26.6%)\n",
      "[INFO]   Fold 4/5 Sujetos VAE actual train: 315, VAE internal val: 79\n",
      "[INFO] Aplicando normalización inter-canal (modo: zscore_offdiag) sobre 3 canales seleccionados.\n",
      "[INFO] Parámetros de normalización se calcularán usando 315 sujetos de entrenamiento.\n",
      "[INFO] Canal 'Pearson_Full_FisherZ_Signed': Off-diag zscore_offdiag (train_params: mean=-0.047, std=0.775)\n",
      "[INFO] Canal 'MI_KNN_Symmetric': Off-diag zscore_offdiag (train_params: mean=0.058, std=0.816)\n",
      "[INFO] Canal 'DistanceCorr': Off-diag zscore_offdiag (train_params: mean=-0.403, std=1.693)\n",
      "[INFO]   Fold 4/5 Usando dispositivo: cuda\n",
      "[INFO]   Fold 4/5 Usando scheduler: CosineAnnealingWarmRestarts (T_0=30)\n",
      "[INFO]   Fold 4/5 Entrenando VAE (Decoder: convtranspose, Encoder Layers: 4)...\n",
      "[INFO]   Fold 4/5 VAE E5/300, TrL: 63614.74 (R: 63508.03, KLD: 173.98), Beta: 0.613, LR: 9.38e-05, ValL: 58289.17 (R: 58242.82, KLD: 75.57)\n",
      "[INFO]   Fold 4/5 VAE E10/300, TrL: 60751.99 (R: 60425.60, KLD: 236.52), Beta: 1.380, LR: 7.60e-05, ValL: 54085.05 (R: 53621.35, KLD: 336.01)\n",
      "[INFO]   Fold 4/5 VAE E15/300, TrL: 55448.58 (R: 54576.21, KLD: 406.38), Beta: 2.147, LR: 5.13e-05, ValL: 50495.98 (R: 49017.65, KLD: 688.66)\n",
      "[INFO]   Fold 4/5 VAE E20/300, TrL: 53082.07 (R: 52174.29, KLD: 311.60), Beta: 2.913, LR: 2.63e-05, ValL: 48578.19 (R: 47619.68, KLD: 329.01)\n",
      "[INFO]   Fold 4/5 VAE E25/300, TrL: 52657.62 (R: 51706.29, KLD: 258.51), Beta: 3.680, LR: 7.70e-06, ValL: 48007.37 (R: 47087.35, KLD: 250.01)\n",
      "[INFO]   Fold 4/5 VAE E30/300, TrL: 52644.12 (R: 51534.11, KLD: 249.63), Beta: 4.447, LR: 5.11e-07, ValL: 48085.16 (R: 46998.07, KLD: 244.47)\n",
      "[INFO]   Fold 4/5 VAE E35/300, TrL: 50240.51 (R: 49367.34, KLD: 189.82), Beta: 4.600, LR: 9.38e-05, ValL: 44956.99 (R: 44082.29, KLD: 190.15)\n",
      "[INFO]   Fold 4/5 VAE E40/300, TrL: 47548.25 (R: 46801.69, KLD: 162.30), Beta: 4.600, LR: 7.60e-05, ValL: 41982.71 (R: 41257.17, KLD: 157.73)\n",
      "[INFO]   Fold 4/5 VAE E45/300, TrL: 45497.26 (R: 44809.99, KLD: 149.41), Beta: 4.600, LR: 5.13e-05, ValL: 39643.28 (R: 39027.54, KLD: 133.86)\n",
      "[INFO]   Fold 4/5 VAE E50/300, TrL: 44463.83 (R: 43812.91, KLD: 141.50), Beta: 4.600, LR: 2.63e-05, ValL: 38522.23 (R: 37950.01, KLD: 124.40)\n",
      "[INFO]   Fold 4/5 VAE E55/300, TrL: 44070.17 (R: 43427.24, KLD: 139.77), Beta: 4.600, LR: 7.70e-06, ValL: 38084.47 (R: 37531.18, KLD: 120.28)\n",
      "[INFO]   Fold 4/5 VAE E60/300, TrL: 43814.20 (R: 43180.13, KLD: 137.84), Beta: 4.600, LR: 5.11e-07, ValL: 37969.70 (R: 37422.34, KLD: 118.99)\n",
      "[INFO]   Fold 4/5 VAE E65/300, TrL: 42333.02 (R: 41698.44, KLD: 137.95), Beta: 4.600, LR: 9.38e-05, ValL: 36140.13 (R: 35519.34, KLD: 134.95)\n",
      "[INFO]   Fold 4/5 VAE E70/300, TrL: 40955.02 (R: 40336.27, KLD: 134.51), Beta: 4.600, LR: 7.60e-05, ValL: 35042.49 (R: 34471.17, KLD: 124.20)\n",
      "[INFO]   Fold 4/5 VAE E75/300, TrL: 40265.57 (R: 39653.00, KLD: 133.17), Beta: 4.600, LR: 5.13e-05, ValL: 34302.64 (R: 33753.15, KLD: 119.45)\n",
      "[INFO]   Fold 4/5 VAE E80/300, TrL: 39177.80 (R: 39067.95, KLD: 179.10), Beta: 0.613, LR: 2.63e-05, ValL: 33399.52 (R: 33299.49, KLD: 163.10)\n",
      "[INFO]   Fold 4/5 VAE E85/300, TrL: 39200.58 (R: 38938.15, KLD: 190.17), Beta: 1.380, LR: 7.70e-06, ValL: 33357.84 (R: 33118.05, KLD: 173.75)\n",
      "[INFO]   Fold 4/5 VAE E90/300, TrL: 39173.61 (R: 38773.08, KLD: 186.58), Beta: 2.147, LR: 5.11e-07, ValL: 33395.11 (R: 33028.69, KLD: 170.70)\n",
      "[INFO]   Fold 4/5 VAE E95/300, TrL: 38720.90 (R: 38230.44, KLD: 168.35), Beta: 2.913, LR: 9.38e-05, ValL: 32849.10 (R: 32383.57, KLD: 159.79)\n",
      "[INFO]   Fold 4/5 VAE E100/300, TrL: 38215.54 (R: 37610.78, KLD: 164.34), Beta: 3.680, LR: 7.60e-05, ValL: 32362.14 (R: 31817.03, KLD: 148.13)\n",
      "[INFO]   Fold 4/5 VAE E105/300, TrL: 37815.06 (R: 37139.59, KLD: 151.91), Beta: 4.447, LR: 5.13e-05, ValL: 32164.13 (R: 31577.53, KLD: 131.92)\n",
      "[INFO]   Fold 4/5 VAE E110/300, TrL: 37407.14 (R: 36718.49, KLD: 149.71), Beta: 4.600, LR: 2.63e-05, ValL: 31854.42 (R: 31303.34, KLD: 119.80)\n",
      "[INFO]   Fold 4/5 VAE E115/300, TrL: 37491.41 (R: 36828.96, KLD: 144.01), Beta: 4.600, LR: 7.70e-06, ValL: 31808.05 (R: 31261.27, KLD: 118.86)\n",
      "[INFO]   Fold 4/5 VAE E120/300, TrL: 37483.66 (R: 36831.26, KLD: 141.83), Beta: 4.600, LR: 5.11e-07, ValL: 31754.52 (R: 31224.33, KLD: 115.26)\n",
      "[INFO]   Fold 4/5 VAE E125/300, TrL: 37058.49 (R: 36399.50, KLD: 143.26), Beta: 4.600, LR: 9.38e-05, ValL: 31488.14 (R: 30917.34, KLD: 124.08)\n",
      "[INFO]   Fold 4/5 VAE E130/300, TrL: 36514.73 (R: 35870.05, KLD: 140.15), Beta: 4.600, LR: 7.60e-05, ValL: 31152.13 (R: 30598.62, KLD: 120.33)\n",
      "[INFO]   Fold 4/5 VAE E135/300, TrL: 36371.41 (R: 35712.37, KLD: 143.27), Beta: 4.600, LR: 5.13e-05, ValL: 30830.33 (R: 30291.64, KLD: 117.11)\n",
      "[INFO]   Fold 4/5 VAE E140/300, TrL: 36123.40 (R: 35467.64, KLD: 142.56), Beta: 4.600, LR: 2.63e-05, ValL: 30661.18 (R: 30144.36, KLD: 112.35)\n",
      "[INFO]   Fold 4/5 VAE E145/300, TrL: 36137.39 (R: 35495.87, KLD: 139.46), Beta: 4.600, LR: 7.70e-06, ValL: 30662.00 (R: 30150.39, KLD: 111.22)\n",
      "[INFO]   Fold 4/5 VAE E150/300, TrL: 36048.74 (R: 35402.42, KLD: 140.50), Beta: 4.600, LR: 5.11e-07, ValL: 30630.33 (R: 30123.24, KLD: 110.24)\n",
      "[INFO]   Fold 4/5 VAE E155/300, TrL: 34948.03 (R: 34815.16, KLD: 216.63), Beta: 0.613, LR: 9.38e-05, ValL: 29930.57 (R: 29811.33, KLD: 194.40)\n",
      "[INFO]   Fold 4/5 VAE E160/300, TrL: 34857.70 (R: 34525.87, KLD: 240.46), Beta: 1.380, LR: 7.60e-05, ValL: 29664.35 (R: 29364.67, KLD: 217.17)\n",
      "[INFO]   Fold 4/5 VAE E165/300, TrL: 34820.07 (R: 34328.14, KLD: 229.16), Beta: 2.147, LR: 5.13e-05, ValL: 29697.67 (R: 29270.99, KLD: 198.77)\n",
      "[INFO]   Fold 4/5 VAE E170/300, TrL: 34742.07 (R: 34125.99, KLD: 211.47), Beta: 2.913, LR: 2.63e-05, ValL: 29758.10 (R: 29232.12, KLD: 180.54)\n",
      "[INFO]   Fold 4/5 VAE E175/300, TrL: 34965.86 (R: 34234.39, KLD: 198.77), Beta: 3.680, LR: 7.70e-06, ValL: 29776.19 (R: 29157.58, KLD: 168.10)\n",
      "[INFO]   Fold 4/5 VAE E180/300, TrL: 34943.08 (R: 34074.32, KLD: 195.37), Beta: 4.447, LR: 5.11e-07, ValL: 29966.32 (R: 29237.83, KLD: 163.83)\n",
      "[INFO]   Fold 4/5 VAE E185/300, TrL: 34905.03 (R: 34184.82, KLD: 156.57), Beta: 4.600, LR: 9.38e-05, ValL: 29722.60 (R: 29103.47, KLD: 134.59)\n",
      "[INFO]   Fold 4/5 VAE E190/300, TrL: 34591.59 (R: 33884.23, KLD: 153.77), Beta: 4.600, LR: 7.60e-05, ValL: 29566.96 (R: 28979.50, KLD: 127.71)\n",
      "[INFO]   Fold 4/5 VAE E195/300, TrL: 34396.66 (R: 33690.32, KLD: 153.55), Beta: 4.600, LR: 5.13e-05, ValL: 29372.17 (R: 28814.37, KLD: 121.26)\n",
      "[INFO]   Fold 4/5 VAE E200/300, TrL: 34321.06 (R: 33607.58, KLD: 155.11), Beta: 4.600, LR: 2.63e-05, ValL: 29249.14 (R: 28690.71, KLD: 121.40)\n",
      "[INFO]   Fold 4/5 VAE E205/300, TrL: 34155.55 (R: 33448.21, KLD: 153.77), Beta: 4.600, LR: 7.70e-06, ValL: 29233.70 (R: 28694.88, KLD: 117.14)\n",
      "[INFO]   Fold 4/5 VAE E210/300, TrL: 34239.56 (R: 33536.25, KLD: 152.89), Beta: 4.600, LR: 5.11e-07, ValL: 29197.84 (R: 28652.25, KLD: 118.61)\n",
      "[INFO]   Fold 4/5 VAE E215/300, TrL: 34082.20 (R: 33348.20, KLD: 159.57), Beta: 4.600, LR: 9.38e-05, ValL: 29119.94 (R: 28560.19, KLD: 121.68)\n",
      "[INFO]   Fold 4/5 VAE E220/300, TrL: 33992.84 (R: 33254.65, KLD: 160.47), Beta: 4.600, LR: 7.60e-05, ValL: 28992.49 (R: 28409.80, KLD: 126.67)\n",
      "[INFO]   Fold 4/5 VAE E225/300, TrL: 33790.45 (R: 33071.46, KLD: 156.30), Beta: 4.600, LR: 5.13e-05, ValL: 28886.97 (R: 28326.39, KLD: 121.87)\n",
      "[INFO]   Fold 4/5 VAE E230/300, TrL: 32996.74 (R: 32884.83, KLD: 182.46), Beta: 0.613, LR: 2.63e-05, ValL: 28305.99 (R: 28212.46, KLD: 152.49)\n",
      "[INFO]   Fold 4/5 VAE E235/300, TrL: 33097.72 (R: 32828.41, KLD: 195.15), Beta: 1.380, LR: 7.70e-06, ValL: 28402.91 (R: 28177.91, KLD: 163.04)\n",
      "[INFO]   Fold 4/5 VAE E240/300, TrL: 33114.62 (R: 32695.77, KLD: 195.12), Beta: 2.147, LR: 5.11e-07, ValL: 28517.54 (R: 28163.59, KLD: 164.88)\n",
      "[INFO]   Fold 4/5 VAE E245/300, TrL: 33224.41 (R: 32650.13, KLD: 197.12), Beta: 2.913, LR: 9.38e-05, ValL: 28604.02 (R: 28133.77, KLD: 161.41)\n",
      "[INFO]   Fold 4/5 VAE E250/300, TrL: 33204.35 (R: 32523.94, KLD: 184.89), Beta: 3.680, LR: 7.60e-05, ValL: 28683.32 (R: 28123.82, KLD: 152.04)\n",
      "[INFO]   Fold 4/5 VAE E255/300, TrL: 33179.27 (R: 32398.94, KLD: 175.49), Beta: 4.447, LR: 5.13e-05, ValL: 28677.90 (R: 28068.32, KLD: 137.09)\n",
      "[INFO]   Fold 4/5 Early stopping VAE en epoch 258. Mejor val_loss: 28302.1613 (época 228)\n",
      "[INFO]   Fold 4/5 VAE final model loaded (best val_loss: 28302.1613).\n",
      "[INFO]   Fold 4/5 Modelo VAE guardado en: resultados_13_paper/fold_4/vae_model_fold_4.pt\n",
      "[INFO] Fold 4/5 Pool Train/Dev (Clasificador) (N=147):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 76 (51.7%)\n",
      "      CN: 71 (48.3%)\n",
      "    Sex:\n",
      "      F: 74 (50.3%)\n",
      "      M: 73 (49.7%)\n",
      "[INFO]   Añadiendo metadatos al clasificador: ['Age', 'Sex']\n",
      "[INFO]   Forma final del set de entrenamiento del clasificador: (147, 514)\n",
      "[INFO]     --- Entrenando Clasificador: xgb ---\n",
      "[XGBoost] ➜  Se usará GPU (device=cuda)\n",
      "[INFO] [SMOTE] ➜ aplicado sólo dentro de folds (imblearn Pipeline).\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para xgb: {'model__gamma': 2.5334131565810813, 'model__n_estimators': 672, 'model__learning_rate': 0.002700911589097508, 'model__max_depth': 5, 'model__subsample': 0.8330062339957954, 'model__colsample_bytree': 0.9087550840721653, 'model__min_child_weight': 4.328911015631313}\n",
      "[INFO]       Modelo final (pipeline) para xgb listo.\n",
      "[INFO]       Resultados Fold 4 (xgb): AUC=0.7953, Bal.Acc=0.8114\n",
      "[INFO]       Pipeline completo de xgb del fold 4 guardado.\n",
      "[INFO]     --- Entrenando Clasificador: svm ---\n",
      "[INFO] [SMOTE] ➜ aplicado sólo dentro de folds (imblearn Pipeline).\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para svm: {'model__estimator__C': 2382.014791857366, 'model__estimator__gamma': 5.819179367044742e-07, 'model__estimator__kernel': 'rbf'}\n",
      "[INFO]       Modelo final (pipeline) para svm listo.\n",
      "[INFO]       Resultados Fold 4 (svm): AUC=0.7778, Bal.Acc=0.7573\n",
      "[INFO]       Pipeline completo de svm del fold 4 guardado.\n",
      "[INFO]     --- Entrenando Clasificador: logreg ---\n",
      "[INFO] [SMOTE] ➜ aplicado sólo dentro de folds (imblearn Pipeline).\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para logreg: {'model__C': 0.008664019358463093}\n",
      "[INFO]       Modelo final (pipeline) para logreg listo.\n",
      "[INFO]       Resultados Fold 4 (logreg): AUC=0.7661, Bal.Acc=0.7836\n",
      "[INFO]       Pipeline completo de logreg del fold 4 guardado.\n",
      "[INFO]     --- Entrenando Clasificador: gb ---\n",
      "[LightGBM] ⚠ No se pudo comprobar la GPU, usando CPU\n",
      "[INFO] [SMOTE] ➜ aplicado sólo dentro de folds (imblearn Pipeline).\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para gb: {'model__estimator__max_depth': 4, 'model__estimator__num_leaves': 993, 'model__estimator__bagging_fraction': 0.7523475349247294, 'model__estimator__feature_fraction': 0.9942877193927231, 'model__estimator__bagging_freq': 7, 'model__estimator__learning_rate': 0.001862971535199364, 'model__estimator__n_estimators': 854, 'model__estimator__min_child_samples': 11, 'model__estimator__min_child_weight': 0.10580176504418662, 'model__estimator__min_split_gain': 0.22724649255735452, 'model__estimator__reg_alpha': 0.004672104486434796, 'model__estimator__reg_lambda': 0.022049042852201894}\n",
      "[INFO]       Modelo final (pipeline) para gb listo.\n",
      "[INFO]       Resultados Fold 4 (gb): AUC=0.7982, Bal.Acc=0.7836\n",
      "[INFO]       Pipeline completo de gb del fold 4 guardado.\n",
      "[INFO]     --- Entrenando Clasificador: mlp ---\n",
      "[INFO] [SMOTE] ➜ aplicado sólo dentro de folds (imblearn Pipeline).\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para mlp: {'model__alpha': 1.1106654191435744e-05, 'model__learning_rate_init': 0.007944928594025487}\n",
      "[INFO]       Modelo final (pipeline) para mlp listo.\n",
      "[INFO]       Resultados Fold 4 (mlp): AUC=0.7632, Bal.Acc=0.7544\n",
      "[INFO]       Pipeline completo de mlp del fold 4 guardado.\n",
      "[INFO]   Fold 4/5 completado en 774.88 segundos.\n",
      "[INFO] --- Iniciando Fold 5/5 ---\n",
      "[INFO] Fold 5/5 Test Set (Clasificador) (N=36):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 19 (52.8%)\n",
      "      CN: 17 (47.2%)\n",
      "    Sex:\n",
      "      F: 18 (50.0%)\n",
      "      M: 18 (50.0%)\n",
      "[INFO] Fold 5/5 Pool Entrenamiento VAE (N=395):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 76 (19.2%)\n",
      "      CN: 72 (18.2%)\n",
      "      MCI: 247 (62.5%)\n",
      "    Sex:\n",
      "      F: 188 (47.6%)\n",
      "      M: 207 (52.4%)\n",
      "    Age_Group:\n",
      "      0: 101 (25.6%)\n",
      "      1: 105 (26.6%)\n",
      "      2: 98 (24.8%)\n",
      "      3: 91 (23.0%)\n",
      "[INFO]   Fold 5/5 VAE val split será estratificado por ['ResearchGroup_Mapped', 'Sex', 'Age_Group'].\n",
      "[INFO] Fold 5/5 Actual Train Set (VAE) (N=316):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 61 (19.3%)\n",
      "      CN: 58 (18.4%)\n",
      "      MCI: 197 (62.3%)\n",
      "    Sex:\n",
      "      F: 151 (47.8%)\n",
      "      M: 165 (52.2%)\n",
      "    Age_Group:\n",
      "      0: 82 (25.9%)\n",
      "      1: 84 (26.6%)\n",
      "      2: 77 (24.4%)\n",
      "      3: 73 (23.1%)\n",
      "[INFO] Fold 5/5 Internal Val Set (VAE) (N=79):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 15 (19.0%)\n",
      "      CN: 14 (17.7%)\n",
      "      MCI: 50 (63.3%)\n",
      "    Sex:\n",
      "      F: 37 (46.8%)\n",
      "      M: 42 (53.2%)\n",
      "    Age_Group:\n",
      "      0: 19 (24.1%)\n",
      "      1: 21 (26.6%)\n",
      "      2: 21 (26.6%)\n",
      "      3: 18 (22.8%)\n",
      "[INFO]   Fold 5/5 Sujetos VAE actual train: 316, VAE internal val: 79\n",
      "[INFO] Aplicando normalización inter-canal (modo: zscore_offdiag) sobre 3 canales seleccionados.\n",
      "[INFO] Parámetros de normalización se calcularán usando 316 sujetos de entrenamiento.\n",
      "[INFO] Canal 'Pearson_Full_FisherZ_Signed': Off-diag zscore_offdiag (train_params: mean=-0.047, std=0.777)\n",
      "[INFO] Canal 'MI_KNN_Symmetric': Off-diag zscore_offdiag (train_params: mean=0.057, std=0.814)\n",
      "[INFO] Canal 'DistanceCorr': Off-diag zscore_offdiag (train_params: mean=-0.407, std=1.702)\n",
      "[INFO]   Fold 5/5 Usando dispositivo: cuda\n",
      "[INFO]   Fold 5/5 Usando scheduler: CosineAnnealingWarmRestarts (T_0=30)\n",
      "[INFO]   Fold 5/5 Entrenando VAE (Decoder: convtranspose, Encoder Layers: 4)...\n",
      "[INFO]   Fold 5/5 VAE E5/300, TrL: 63504.49 (R: 63396.76, KLD: 175.65), Beta: 0.613, LR: 9.38e-05, ValL: 62142.10 (R: 62092.60, KLD: 80.71)\n",
      "[INFO]   Fold 5/5 VAE E10/300, TrL: 59677.36 (R: 59273.52, KLD: 292.64), Beta: 1.380, LR: 7.60e-05, ValL: 56785.64 (R: 56039.13, KLD: 540.95)\n",
      "[INFO]   Fold 5/5 VAE E15/300, TrL: 54262.27 (R: 53329.97, KLD: 434.30), Beta: 2.147, LR: 5.13e-05, ValL: 53561.45 (R: 52286.42, KLD: 593.96)\n",
      "[INFO]   Fold 5/5 VAE E20/300, TrL: 52179.37 (R: 51342.21, KLD: 287.36), Beta: 2.913, LR: 2.63e-05, ValL: 51330.80 (R: 50454.70, KLD: 300.72)\n",
      "[INFO]   Fold 5/5 VAE E25/300, TrL: 51522.81 (R: 50584.79, KLD: 254.90), Beta: 3.680, LR: 7.70e-06, ValL: 50831.32 (R: 49835.74, KLD: 270.54)\n",
      "[INFO]   Fold 5/5 VAE E30/300, TrL: 51612.98 (R: 50516.89, KLD: 246.50), Beta: 4.447, LR: 5.11e-07, ValL: 50829.88 (R: 49689.07, KLD: 256.55)\n",
      "[INFO]   Fold 5/5 VAE E35/300, TrL: 48750.00 (R: 47945.38, KLD: 174.92), Beta: 4.600, LR: 9.38e-05, ValL: 47502.67 (R: 46664.21, KLD: 182.27)\n",
      "[INFO]   Fold 5/5 VAE E40/300, TrL: 46176.50 (R: 45513.42, KLD: 144.15), Beta: 4.600, LR: 7.60e-05, ValL: 44662.42 (R: 44037.93, KLD: 135.76)\n",
      "[INFO]   Fold 5/5 VAE E45/300, TrL: 44496.96 (R: 43872.02, KLD: 135.86), Beta: 4.600, LR: 5.13e-05, ValL: 43152.44 (R: 42589.21, KLD: 122.44)\n",
      "[INFO]   Fold 5/5 VAE E50/300, TrL: 43659.06 (R: 43061.29, KLD: 129.95), Beta: 4.600, LR: 2.63e-05, ValL: 42249.45 (R: 41728.94, KLD: 113.16)\n",
      "[INFO]   Fold 5/5 VAE E55/300, TrL: 43244.80 (R: 42652.61, KLD: 128.74), Beta: 4.600, LR: 7.70e-06, ValL: 41971.95 (R: 41456.49, KLD: 112.06)\n",
      "[INFO]   Fold 5/5 VAE E60/300, TrL: 43161.30 (R: 42574.57, KLD: 127.55), Beta: 4.600, LR: 5.11e-07, ValL: 41891.65 (R: 41400.98, KLD: 106.67)\n",
      "[INFO]   Fold 5/5 VAE E65/300, TrL: 41971.26 (R: 41367.93, KLD: 131.16), Beta: 4.600, LR: 9.38e-05, ValL: 40313.06 (R: 39773.73, KLD: 117.25)\n",
      "[INFO]   Fold 5/5 VAE E70/300, TrL: 40874.69 (R: 40299.02, KLD: 125.15), Beta: 4.600, LR: 7.60e-05, ValL: 39409.60 (R: 38860.87, KLD: 119.29)\n",
      "[INFO]   Fold 5/5 VAE E75/300, TrL: 39989.52 (R: 39420.70, KLD: 123.65), Beta: 4.600, LR: 5.13e-05, ValL: 38745.61 (R: 38264.20, KLD: 104.66)\n",
      "[INFO]   Fold 5/5 VAE E80/300, TrL: 39102.69 (R: 38999.51, KLD: 168.22), Beta: 0.613, LR: 2.63e-05, ValL: 37855.24 (R: 37754.85, KLD: 163.67)\n",
      "[INFO]   Fold 5/5 VAE E85/300, TrL: 39104.71 (R: 38861.84, KLD: 176.00), Beta: 1.380, LR: 7.70e-06, ValL: 37780.36 (R: 37550.67, KLD: 166.44)\n",
      "[INFO]   Fold 5/5 VAE E90/300, TrL: 39069.67 (R: 38698.48, KLD: 172.92), Beta: 2.147, LR: 5.11e-07, ValL: 37881.46 (R: 37525.76, KLD: 165.70)\n",
      "[INFO]   Fold 5/5 VAE E95/300, TrL: 38539.11 (R: 38075.25, KLD: 159.22), Beta: 2.913, LR: 9.38e-05, ValL: 37227.84 (R: 36798.04, KLD: 147.53)\n",
      "[INFO]   Fold 5/5 VAE E100/300, TrL: 38009.20 (R: 37427.50, KLD: 158.07), Beta: 3.680, LR: 7.60e-05, ValL: 36737.78 (R: 36219.16, KLD: 140.93)\n",
      "[INFO]   Fold 5/5 VAE E105/300, TrL: 37768.76 (R: 37133.85, KLD: 142.78), Beta: 4.447, LR: 5.13e-05, ValL: 36338.73 (R: 35801.78, KLD: 120.75)\n",
      "[INFO]   Fold 5/5 VAE E110/300, TrL: 37386.47 (R: 36749.95, KLD: 138.37), Beta: 4.600, LR: 2.63e-05, ValL: 36288.10 (R: 35749.58, KLD: 117.07)\n",
      "[INFO]   Fold 5/5 VAE E115/300, TrL: 37223.70 (R: 36609.16, KLD: 133.60), Beta: 4.600, LR: 7.70e-06, ValL: 36075.34 (R: 35575.67, KLD: 108.62)\n",
      "[INFO]   Fold 5/5 VAE E120/300, TrL: 37170.31 (R: 36547.59, KLD: 135.38), Beta: 4.600, LR: 5.11e-07, ValL: 36030.13 (R: 35522.51, KLD: 110.35)\n",
      "[INFO]   Fold 5/5 VAE E125/300, TrL: 36895.87 (R: 36248.44, KLD: 140.75), Beta: 4.600, LR: 9.38e-05, ValL: 35739.89 (R: 35181.81, KLD: 121.32)\n",
      "[INFO]   Fold 5/5 VAE E130/300, TrL: 36609.57 (R: 35947.98, KLD: 143.82), Beta: 4.600, LR: 7.60e-05, ValL: 35373.19 (R: 34810.35, KLD: 122.36)\n",
      "[INFO]   Fold 5/5 VAE E135/300, TrL: 36210.41 (R: 35576.46, KLD: 137.81), Beta: 4.600, LR: 5.13e-05, ValL: 35200.99 (R: 34682.96, KLD: 112.62)\n",
      "[INFO]   Fold 5/5 VAE E140/300, TrL: 36028.57 (R: 35365.24, KLD: 144.20), Beta: 4.600, LR: 2.63e-05, ValL: 34965.16 (R: 34434.35, KLD: 115.39)\n",
      "[INFO]   Fold 5/5 VAE E145/300, TrL: 35928.98 (R: 35298.01, KLD: 137.17), Beta: 4.600, LR: 7.70e-06, ValL: 34945.80 (R: 34440.63, KLD: 109.82)\n",
      "[INFO]   Fold 5/5 VAE E150/300, TrL: 35846.08 (R: 35219.68, KLD: 136.18), Beta: 4.600, LR: 5.11e-07, ValL: 34925.11 (R: 34418.10, KLD: 110.22)\n",
      "[INFO]   Fold 5/5 VAE E155/300, TrL: 34979.46 (R: 34845.49, KLD: 218.43), Beta: 0.613, LR: 9.38e-05, ValL: 34228.60 (R: 34104.62, KLD: 202.15)\n",
      "[INFO]   Fold 5/5 VAE E160/300, TrL: 34731.71 (R: 34397.35, KLD: 242.29), Beta: 1.380, LR: 7.60e-05, ValL: 34011.59 (R: 33711.66, KLD: 217.34)\n",
      "[INFO]   Fold 5/5 VAE E165/300, TrL: 34831.20 (R: 34351.89, KLD: 223.28), Beta: 2.147, LR: 5.13e-05, ValL: 34052.76 (R: 33614.68, KLD: 204.07)\n",
      "[INFO]   Fold 5/5 VAE E170/300, TrL: 34784.09 (R: 34187.81, KLD: 204.67), Beta: 2.913, LR: 2.63e-05, ValL: 33964.45 (R: 33444.40, KLD: 178.51)\n",
      "[INFO]   Fold 5/5 VAE E175/300, TrL: 34829.35 (R: 34116.14, KLD: 193.81), Beta: 3.680, LR: 7.70e-06, ValL: 34062.55 (R: 33447.61, KLD: 167.10)\n",
      "[INFO]   Fold 5/5 VAE E180/300, TrL: 34985.80 (R: 34148.86, KLD: 188.22), Beta: 4.447, LR: 5.11e-07, ValL: 34177.09 (R: 33452.35, KLD: 162.99)\n",
      "[INFO]   Fold 5/5 VAE E185/300, TrL: 34759.13 (R: 34074.18, KLD: 148.90), Beta: 4.600, LR: 9.38e-05, ValL: 34005.68 (R: 33402.78, KLD: 131.06)\n",
      "[INFO]   Fold 5/5 VAE E190/300, TrL: 34530.51 (R: 33836.48, KLD: 150.88), Beta: 4.600, LR: 7.60e-05, ValL: 33823.94 (R: 33249.65, KLD: 124.84)\n",
      "[INFO]   Fold 5/5 VAE E195/300, TrL: 34347.56 (R: 33680.53, KLD: 145.01), Beta: 4.600, LR: 5.13e-05, ValL: 33723.20 (R: 33185.21, KLD: 116.95)\n",
      "[INFO]   Fold 5/5 VAE E200/300, TrL: 34296.50 (R: 33624.50, KLD: 146.09), Beta: 4.600, LR: 2.63e-05, ValL: 33525.36 (R: 32986.25, KLD: 117.20)\n",
      "[INFO]   Fold 5/5 VAE E205/300, TrL: 34195.37 (R: 33529.16, KLD: 144.83), Beta: 4.600, LR: 7.70e-06, ValL: 33524.83 (R: 32994.10, KLD: 115.38)\n",
      "[INFO]   Fold 5/5 VAE E210/300, TrL: 34039.20 (R: 33368.02, KLD: 145.91), Beta: 4.600, LR: 5.11e-07, ValL: 33527.86 (R: 33000.09, KLD: 114.73)\n",
      "[INFO]   Fold 5/5 VAE E215/300, TrL: 34117.02 (R: 33428.20, KLD: 149.75), Beta: 4.600, LR: 9.38e-05, ValL: 33602.72 (R: 32987.99, KLD: 133.64)\n",
      "[INFO]   Fold 5/5 VAE E220/300, TrL: 33917.66 (R: 33222.80, KLD: 151.06), Beta: 4.600, LR: 7.60e-05, ValL: 33334.22 (R: 32762.01, KLD: 124.39)\n",
      "[INFO]   Fold 5/5 VAE E225/300, TrL: 33729.70 (R: 33026.25, KLD: 152.92), Beta: 4.600, LR: 5.13e-05, ValL: 33221.62 (R: 32660.20, KLD: 122.05)\n",
      "[INFO]   Fold 5/5 VAE E230/300, TrL: 32907.67 (R: 32799.51, KLD: 176.35), Beta: 0.613, LR: 2.63e-05, ValL: 32523.97 (R: 32430.47, KLD: 152.44)\n",
      "[INFO]   Fold 5/5 VAE E235/300, TrL: 33079.99 (R: 32818.54, KLD: 189.45), Beta: 1.380, LR: 7.70e-06, ValL: 32604.58 (R: 32380.35, KLD: 162.49)\n",
      "[INFO]   Fold 5/5 VAE E240/300, TrL: 33174.56 (R: 32761.13, KLD: 192.59), Beta: 2.147, LR: 5.11e-07, ValL: 32698.61 (R: 32342.40, KLD: 165.94)\n",
      "[INFO]   Fold 5/5 VAE E245/300, TrL: 33195.85 (R: 32643.44, KLD: 189.61), Beta: 2.913, LR: 9.38e-05, ValL: 32826.57 (R: 32355.26, KLD: 161.78)\n",
      "[INFO]   Fold 5/5 VAE E250/300, TrL: 33153.90 (R: 32488.02, KLD: 180.95), Beta: 3.680, LR: 7.60e-05, ValL: 32753.10 (R: 32195.70, KLD: 151.47)\n",
      "[INFO]   Fold 5/5 VAE E255/300, TrL: 33170.98 (R: 32421.87, KLD: 168.47), Beta: 4.447, LR: 5.13e-05, ValL: 32793.30 (R: 32186.20, KLD: 136.53)\n",
      "[INFO]   Fold 5/5 VAE E260/300, TrL: 33157.55 (R: 32410.91, KLD: 162.31), Beta: 4.600, LR: 2.63e-05, ValL: 32751.34 (R: 32151.10, KLD: 130.49)\n",
      "[INFO]   Fold 5/5 Early stopping VAE en epoch 261. Mejor val_loss: 32511.6706 (época 231)\n",
      "[INFO]   Fold 5/5 VAE final model loaded (best val_loss: 32511.6706).\n",
      "[INFO]   Fold 5/5 Modelo VAE guardado en: resultados_13_paper/fold_5/vae_model_fold_5.pt\n",
      "[INFO] Fold 5/5 Pool Train/Dev (Clasificador) (N=148):\n",
      "    ResearchGroup_Mapped:\n",
      "      AD: 76 (51.4%)\n",
      "      CN: 72 (48.6%)\n",
      "    Sex:\n",
      "      F: 74 (50.0%)\n",
      "      M: 74 (50.0%)\n",
      "[INFO]   Añadiendo metadatos al clasificador: ['Age', 'Sex']\n",
      "[INFO]   Forma final del set de entrenamiento del clasificador: (148, 514)\n",
      "[INFO]     --- Entrenando Clasificador: xgb ---\n",
      "[XGBoost] ➜  Se usará GPU (device=cuda)\n",
      "[INFO] [SMOTE] ➜ aplicado sólo dentro de folds (imblearn Pipeline).\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para xgb: {'model__gamma': 1.656617799700748, 'model__n_estimators': 815, 'model__learning_rate': 0.044481322463378686, 'model__max_depth': 12, 'model__subsample': 0.4828333210611153, 'model__colsample_bytree': 0.7168319844064686, 'model__min_child_weight': 1.2478882717282527}\n",
      "[INFO]       Modelo final (pipeline) para xgb listo.\n",
      "[INFO]       Resultados Fold 5 (xgb): AUC=0.7090, Bal.Acc=0.5805\n",
      "[INFO]       Pipeline completo de xgb del fold 5 guardado.\n",
      "[INFO]     --- Entrenando Clasificador: svm ---\n",
      "[INFO] [SMOTE] ➜ aplicado sólo dentro de folds (imblearn Pipeline).\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para svm: {'model__estimator__C': 1.5440375683675482, 'model__estimator__gamma': 0.00030182913555515997, 'model__estimator__kernel': 'rbf'}\n",
      "[INFO]       Modelo final (pipeline) para svm listo.\n",
      "[INFO]       Resultados Fold 5 (svm): AUC=0.7353, Bal.Acc=0.6393\n",
      "[INFO]       Pipeline completo de svm del fold 5 guardado.\n",
      "[INFO]     --- Entrenando Clasificador: logreg ---\n",
      "[INFO] [SMOTE] ➜ aplicado sólo dentro de folds (imblearn Pipeline).\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para logreg: {'model__C': 0.003972811044211142}\n",
      "[INFO]       Modelo final (pipeline) para logreg listo.\n",
      "[INFO]       Resultados Fold 5 (logreg): AUC=0.7957, Bal.Acc=0.7214\n",
      "[INFO]       Pipeline completo de logreg del fold 5 guardado.\n",
      "[INFO]     --- Entrenando Clasificador: gb ---\n",
      "[LightGBM] ⚠ No se pudo comprobar la GPU, usando CPU\n",
      "[INFO] [SMOTE] ➜ aplicado sólo dentro de folds (imblearn Pipeline).\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para gb: {'model__estimator__max_depth': 11, 'model__estimator__num_leaves': 954, 'model__estimator__bagging_fraction': 0.6158002002786872, 'model__estimator__feature_fraction': 0.9483114026534032, 'model__estimator__bagging_freq': 2, 'model__estimator__learning_rate': 0.0018305599688229067, 'model__estimator__n_estimators': 900, 'model__estimator__min_child_samples': 18, 'model__estimator__min_child_weight': 0.0065783009618002675, 'model__estimator__min_split_gain': 0.2043028626710745, 'model__estimator__reg_alpha': 0.022540764583463647, 'model__estimator__reg_lambda': 0.1519411041973487}\n",
      "[INFO]       Modelo final (pipeline) para gb listo.\n",
      "[INFO]       Resultados Fold 5 (gb): AUC=0.6858, Bal.Acc=0.6687\n",
      "[INFO]       Pipeline completo de gb del fold 5 guardado.\n",
      "[INFO]     --- Entrenando Clasificador: mlp ---\n",
      "[INFO] [SMOTE] ➜ aplicado sólo dentro de folds (imblearn Pipeline).\n",
      "[INFO]       Usando Optuna MedianPruner para acelerar la búsqueda.\n",
      "[INFO]       Mejores HPs para mlp: {'model__alpha': 0.0015963672941156927, 'model__learning_rate_init': 0.007917218974610013}\n",
      "[INFO]       Modelo final (pipeline) para mlp listo.\n",
      "[INFO]       Resultados Fold 5 (mlp): AUC=0.8080, Bal.Acc=0.7508\n",
      "[INFO]       Pipeline completo de mlp del fold 5 guardado.\n",
      "[INFO]   Fold 5/5 completado en 788.27 segundos.\n",
      "[INFO] \n",
      "--- Resumen de Rendimiento para Clasificador: xgb (Promedio sobre Folds Externos) ---\n",
      "[INFO] Auc                 : 0.7821 +/- 0.0515\n",
      "[INFO] Pr_auc              : 0.7831 +/- 0.0667\n",
      "[INFO] Accuracy            : 0.7113 +/- 0.0865\n",
      "[INFO] Balanced_accuracy   : 0.7108 +/- 0.0876\n",
      "[INFO] Sensitivity         : 0.7158 +/- 0.0798\n",
      "[INFO] Specificity         : 0.7059 +/- 0.1166\n",
      "[INFO] F1_score            : 0.7199 +/- 0.0779\n",
      "[INFO] \n",
      "--- Resumen de Rendimiento para Clasificador: svm (Promedio sobre Folds Externos) ---\n",
      "[INFO] Auc                 : 0.7897 +/- 0.0511\n",
      "[INFO] Pr_auc              : 0.7819 +/- 0.0623\n",
      "[INFO] Accuracy            : 0.7116 +/- 0.0488\n",
      "[INFO] Balanced_accuracy   : 0.7112 +/- 0.0487\n",
      "[INFO] Sensitivity         : 0.7263 +/- 0.0942\n",
      "[INFO] Specificity         : 0.6961 +/- 0.0951\n",
      "[INFO] F1_score            : 0.7209 +/- 0.0515\n",
      "[INFO] \n",
      "--- Resumen de Rendimiento para Clasificador: logreg (Promedio sobre Folds Externos) ---\n",
      "[INFO] Auc                 : 0.8135 +/- 0.0384\n",
      "[INFO] Pr_auc              : 0.8037 +/- 0.0616\n",
      "[INFO] Accuracy            : 0.7553 +/- 0.0440\n",
      "[INFO] Balanced_accuracy   : 0.7545 +/- 0.0433\n",
      "[INFO] Sensitivity         : 0.7789 +/- 0.0781\n",
      "[INFO] Specificity         : 0.7301 +/- 0.0276\n",
      "[INFO] F1_score            : 0.7655 +/- 0.0485\n",
      "[INFO] \n",
      "--- Resumen de Rendimiento para Clasificador: gb (Promedio sobre Folds Externos) ---\n",
      "[INFO] Auc                 : 0.7515 +/- 0.0451\n",
      "[INFO] Pr_auc              : 0.7469 +/- 0.0656\n",
      "[INFO] Accuracy            : 0.7063 +/- 0.0462\n",
      "[INFO] Balanced_accuracy   : 0.7071 +/- 0.0459\n",
      "[INFO] Sensitivity         : 0.6842 +/- 0.0744\n",
      "[INFO] Specificity         : 0.7301 +/- 0.0734\n",
      "[INFO] F1_score            : 0.7055 +/- 0.0484\n",
      "[INFO] \n",
      "--- Resumen de Rendimiento para Clasificador: mlp (Promedio sobre Folds Externos) ---\n",
      "[INFO] Auc                 : 0.8119 +/- 0.0421\n",
      "[INFO] Pr_auc              : 0.8304 +/- 0.0465\n",
      "[INFO] Accuracy            : 0.7446 +/- 0.0304\n",
      "[INFO] Balanced_accuracy   : 0.7431 +/- 0.0297\n",
      "[INFO] Sensitivity         : 0.8000 +/- 0.0686\n",
      "[INFO] Specificity         : 0.6863 +/- 0.0438\n",
      "[INFO] F1_score            : 0.7629 +/- 0.0343\n",
      "[INFO] Resultados detallados de todos los clasificadores guardados en: resultados_13_paper/all_folds_metrics_MULTI_xgb_vaeconvtranspose4l_ld512_beta4.6_normzscore_offdiag_ch3sel_intFCquarter_drop0.2_ln0_outer5x1_scoreroc_auc.csv\n",
      "[INFO] Sumario estadístico de métricas (por clasificador) guardado en: resultados_13_paper/summary_metrics_MULTI_xgb_vaeconvtranspose4l_ld512_beta4.6_normzscore_offdiag_ch3sel_intFCquarter_drop0.2_ln0_outer5x1_scoreroc_auc.txt\n",
      "[INFO] Pipeline completo en 3154.81 segundos.\n",
      "[INFO] --- Consideraciones Finales ---\n",
      "[INFO] Normalización: 'zscore_offdiag'. Activación VAE: 'tanh'. Asegurar compatibilidad.\n"
     ]
    }
   ],
   "source": [
    "!python serentipia9.py \\\n",
    "  --global_tensor_path /home/diego/Escritorio/limpio/AAL3_dynamicROIs_fmri_tensor_NeuroEnhanced_v6.5.17_AAL3_131ROIs_OMST_GCE_Signed_GrangerLag1_ChNorm_ROIreorderedYeo17_ParallelTuned/GLOBAL_TENSOR_from_AAL3_dynamicROIs_fmri_tensor_NeuroEnhanced_v6.5.17_AAL3_131ROIs_OMST_GCE_Signed_GrangerLag1_ChNorm_ROIreorderedYeo17_ParallelTuned.npz \\\n",
    "  --metadata_path /home/diego/Escritorio/limpio/SubjectsData_AAL3_procesado.csv \\\n",
    "  --roi_order_path /home/diego/Escritorio/limpio/roi_order_131.npy \\\n",
    "  --output_dir ./resultados_13_paper \\\n",
    "  --channels_to_use 1 2 5 \\\n",
    "  --classifier_types xgb svm logreg gb mlp \\\n",
    "  --classifier_calibrate \\\n",
    "  --outer_folds 5 \\\n",
    "  --repeated_outer_folds_n_repeats 1 \\\n",
    "  --epochs_vae 300 \\\n",
    "  --early_stopping_patience_vae 30 \\\n",
    "  --cyclical_beta_n_cycles 4 \\\n",
    "  --lr_scheduler_type cosine_warm --lr_scheduler_T0 30 \\\n",
    "  --lr_scheduler_eta_min 5e-7 \\\n",
    "  --batch_size 64 \\\n",
    "  --beta_vae 4.6 \\\n",
    "  --dropout_rate_vae 0.2 \\\n",
    "  --latent_dim 512 \\\n",
    "  --n_jobs_gridsearch 8 \\\n",
    "  --metadata_features Age Sex \\\n",
    "  --use_smote \\\n",
    "  --use_optuna_pruner \\\n",
    "  --classifier_use_class_weight \\\n",
    "  --save_fold_artefacts \\\n",
    "  --gridsearch_scoring roc_auc \\\n",
    "  --save_vae_training_history\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "serentipia_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
